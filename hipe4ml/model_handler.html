<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>hipe4ml.model_handler API documentation</title>
<meta name="description" content="Module containing the class used for wrapping the models from different
ML libraries to build a new model with common methods" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>hipe4ml.model_handler</code></h1>
</header>
<section id="section-intro">
<p>Module containing the class used for wrapping the models from different
ML libraries to build a new model with common methods</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#34;&#34;&#34;
Module containing the class used for wrapping the models from different
ML libraries to build a new model with common methods
&#34;&#34;&#34;
from copy import deepcopy
import inspect
import pickle

import numpy as np
import optuna
from sklearn.metrics import roc_auc_score, mean_squared_error
from sklearn.model_selection import cross_val_score

import hipe4ml.tree_handler


class ModelHandler:
    &#34;&#34;&#34;
    Class used for wrapping the models from different ML libraries to
    build a new model with common methods. Currently LightGBM, XGBoost
    (through their sklearn interface) and sklearn models are supported.

    Parameters
    -------------------------------------------------
    input_model: XGBoost, LightGBM or sklearn model

    training_columns: list
        Contains the name of the features used for the training.
        Example: [&#39;dEdx&#39;, &#39;pT&#39;, &#39;ct&#39;]

    model_params: dict
        Model hyper-parameter values. For
        example (XGBoost): max_depth, learning_rate,
        n_estimators, gamma, min_child_weight, ...

    task_type: str
        Task type of the model: &#39;classification&#39; or &#39;regression&#39;
    &#34;&#34;&#34;

    def __init__(self, input_model=None, training_columns=None, model_params=None, task_type=&#39;classification&#39;):
        self.model = input_model
        self.training_columns = training_columns
        self.model_params = model_params
        self._n_classes = None
        self._task_type = task_type
        if self._task_type not in [&#39;classification&#39;, &#39;regression&#39;]:
            raise ValueError(
                &#34;Task type must be either &#39;classification&#39; or &#39;regression&#39;&#34;)

        if self.model is not None:
            self.model_string = inspect.getmodule(
                self.model).__name__.partition(&#39;.&#39;)[0]

            if self.model_string not in [&#34;xgboost&#34;, &#34;lightgbm&#34;, &#34;sklearn&#34;]:
                raise ValueError(
                    &#34;Model must be either &#39;xgboost&#39;, &#39;lightgbm&#39; or &#39;sklearn&#39;&#34;)

            if self.model_params is None:
                self.model_params = self.model.get_params()
            else:
                self.model.set_params(**self.model_params)

    def set_model_params(self, model_params):
        &#34;&#34;&#34;
        Set the model (hyper-)parameters

        Parameters
        ------------------------------------
        model_params: dict
            Model hyper-parameter values. For
            example (XGBoost): max_depth, learning_rate,
            n_estimators, gamma, min_child_weight, ...
        &#34;&#34;&#34;
        self.model_params = model_params
        self.model.set_params(**self.model_params)

    def get_model_params(self):
        &#34;&#34;&#34;
        Get the model (hyper-)parameters

        Returns
        ------------------------------------
        out: dict
            Model hyper-parameter values. For
            example (XGBoost): max_depth, learning_rate,
            n_estimators, gamma, min_child_weight, ...
        &#34;&#34;&#34;
        return self.model.get_params()

    def set_training_columns(self, training_columns):
        &#34;&#34;&#34;
        Set the features used for the training process

        Parameters
        ------------------------------------
        training_columns: list
            Contains the name of the features used for the training.
            Example: [&#39;dEdx&#39;, &#39;pT&#39;, &#39;ct&#39;]
        &#34;&#34;&#34;
        self.training_columns = training_columns

    def get_training_columns(self):
        &#34;&#34;&#34;
        Get the features used for the training process

        Returns
        ------------------------------------
        out: list
            Names of the features used for the training.
            Example: [&#39;dEdx&#39;, &#39;pT&#39;, &#39;ct&#39;]
        &#34;&#34;&#34;

        return self.training_columns

    def get_original_model(self):
        &#34;&#34;&#34;
        Get the original unwrapped model

        Returns
        ---------------------------
        out: XGBoost, LGBM or sklearn model
        &#34;&#34;&#34;
        return self.model

    def get_model_module(self):
        &#34;&#34;&#34;
        Get the string containing the name
        of the model module

        Returns
        ---------------------------
        out: str
            Name of the model module
        &#34;&#34;&#34;
        return self.model_string

    def get_n_classes(self):
        &#34;&#34;&#34;
        Get the number of classes

        Returns
        ---------------------------
        out: int
            Number of classes
        &#34;&#34;&#34;
        return self._n_classes

    def get_task_type(self):
        &#34;&#34;&#34;
        Get the task type of the model

        Returns
        ---------------------------
        out: str
            Task type of the model: &#39;classification&#39; or &#39;regression&#39;
        &#34;&#34;&#34;
        return self._task_type

    def fit(self, x_train, y_train, **kwargs):
        &#34;&#34;&#34;
        Fit Model

        Parameters
        ---------------------------
        x_train: array-like, sparse matrix
            Training data

        y_train: array-like, sparse matrix
            Target data

        **kwargs:
            Extra kwargs passed on to model.fit() method
        &#34;&#34;&#34;
        if self._task_type == &#39;classification&#39;:
            n_classes = len(np.unique(y_train))
            self._n_classes = n_classes
        if self.training_columns is None:
            self.training_columns = list(x_train.columns)

        self.model.fit(x_train[self.training_columns], y_train, **kwargs)

    def predict(self, x_test, output_margin=True, **kwargs):
        &#34;&#34;&#34;
        Return model prediction for the array x_test
        Parameters
        --------------------------------------
        x_test: hipe4ml tree_handler, array-like, sparse matrix
            The input sample.

        output_margin: bool
            Whether to output the raw untransformed margin value. If False model
            probabilities are returned. Not used when task_type is &#39;regression&#39;.

        **kwargs:
            Extra kwargs passed on to the following model prediction function:
            if (task_type == &#39;classification&#39;)
            - predict() (XGBoost and LGBM) or decision_function() (sklearn) if output_margin==True
            - predict_proba() if output_margin==False
            if (task_type == &#39;regression&#39;)
            - predict()

        Returns
        ---------------------------------------
        out: numpy array
            Model predictions
        &#34;&#34;&#34;
        if isinstance(x_test, hipe4ml.tree_handler.TreeHandler):
            x_test = x_test.get_data_frame()

        x_test = x_test[self.training_columns]

        # regression
        if self._task_type == &#39;regression&#39;:
            return self.model.predict(x_test, **kwargs)

        # classification
        if output_margin:
            if self.model_string == &#39;xgboost&#39;:
                return self.model.predict(x_test, output_margin=True, **kwargs)
            if self.model_string == &#39;lightgbm&#39;:
                return self.model.predict(x_test, raw_score=True, **kwargs)
            if self.model_string == &#39;sklearn&#39;:
                if not hasattr(self.model, &#39;decision_function&#39;):
                    raise ValueError(
                        &#34;This Model does not support a decision_function(): use output_margin=False&#34;)
                return self.model.decision_function(x_test, **kwargs).ravel()

        pred = self.model.predict_proba(x_test, **kwargs)
        # in case of binary classification return only the scores of
        # the signal class
        if pred.shape[1] &lt;= 2:
            pred = pred[:, 1]
        return pred

    def train_test_model(self, data, return_prediction=False, output_margin=False, average=&#39;macro&#39;,
                         multi_class_opt=&#39;raise&#39;, **kwargs):
        &#34;&#34;&#34;
        Perform the training and the testing of the model. The model performance is estimated
        using the ROC AUC metric for classification and the MSE for regression.

        Parameters
        ----------------------------------------------
        data: list
            Contains respectively: training
            set dataframe, training label array,
            test set dataframe, test label array

        return_prediction: bool
            If True Model predictions on the test set are
            returned

        output_margin: bool
            Whether to output the raw untransformed margin value. If False model
            probabilities are returned. Not used when task_type is &#39;regression&#39;.

        average: string
            Option for the average of ROC AUC scores used only in case of multi-classification.
            You can choose between &#39;macro&#39; and &#39;weighted&#39;. For more information see
            https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html#sklearn.metrics.roc_auc_score

        multi_class_opt: string
            Option to compute ROC AUC scores used only in case of multi-classification.
            The one-vs-one &#39;ovo&#39; and one-vs-rest &#39;ovr&#39; approaches are available

        **kwargs: dict
            Extra kwargs passed on to the model fit method

        Returns
        ---------------------------------------
        out: numpy array or None
            If return_prediction==True, Model predictions on the test set are
            returned

        &#34;&#34;&#34;

        # get number of classes
        n_classes = len(np.unique(data[1]))
        self._n_classes = n_classes
        print(&#39;==============================&#39;)
        print(f&#34;Training {self.model_string} model for {self._task_type}&#34;)
        if self._task_type == &#39;classification&#39;:
            print(&#39;Number of detected classes:&#39;, n_classes)

        # final training with the optimized hyperparams
        print(&#39;Training the model: ...&#39;)
        self.fit(data[0], data[1], **kwargs)
        print(&#39;Training the model: Done!&#39;)
        print(&#39;Testing the model: ...&#39;)
        y_pred = self.predict(data[2], output_margin=output_margin)
        if self._task_type == &#39;classification&#39;:
            roc_score = roc_auc_score(
                data[3], y_pred, average=average, multi_class=multi_class_opt)
            print(f&#39;ROC_AUC_score: {roc_score:.6f}&#39;)
        else:
            mse_score = mean_squared_error(data[3], y_pred)
            print(f&#39;Mean squared error: {mse_score:.6f}&#39;)
        print(&#39;Testing the model: Done!&#39;)
        print(&#39;==============================&#39;)
        if return_prediction:
            return y_pred
        return None

    def optimize_params_optuna(self, data, hyperparams_ranges, cross_val_scoring, nfold=5, direction=&#39;maximize&#39;,
                               optuna_sampler=None, resume_study=None, save_study=None, **kwargs):
        &#34;&#34;&#34;
        Perform hyperparameter optimization of ModelHandler using the Optuna module.
        The model hyperparameters are automatically set as the ones that provided the
        best result during the optimization.

        Parameters
        ------------------------------------------------------
        data: list
            Contains respectively: training
            set dataframe, training label array,
            test set dataframe, test label array

        hyperparams_ranges: dict
            Hyperparameter ranges (in tuples or list). If a parameter is not
            in a tuple or a list it will be considered constant.
            Important: the type of the params must be preserved
            when passing the ranges.
            For example:
            dict={
                &#39;max_depth&#39;:(10,100)
                &#39;learning_rate&#39;: (0.01,0.03)
                &#39;n_jobs&#39;: 8
            }

        cross_val_scoring: string, callable or None
            Score metrics used for the cross-validation.
            A string (see sklearn model evaluation documentation:
            https://scikit-learn.org/stable/modules/model_evaluation.html)
            or a scorer callable object / function with signature scorer(estimator, X, y)
            which should return only a single value.
            In binary classification &#39;roc_auc&#39; is suggested.
            In multi-classification one between ‘roc_auc_ovr’, ‘roc_auc_ovo’,
            ‘roc_auc_ovr_weighted’ and ‘roc_auc_ovo_weighted’ is suggested.
            For more information see
            https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter

        direction: str
            The direction of optimization. Either &#39;maximize&#39; or &#39;minimize&#39;.
            (e.g. for the metric &#39;roc_auc&#39; the direction is &#39;maximize&#39;)

        optuna_sampler: optuna.samplers.BaseSampler
            Sampler to be used for the optuna (maxi-)minimisation.
            If None, default TPESampler is used. For more information see:
            https://optuna.readthedocs.io/en/stable/reference/samplers.html

        nfold: int
            Number of folds to calculate the cross validation error

        resume_study: str
            A string indicating the filename of the study to be resumed.
            If None, the study is not resumed.

        save_study: str
            A string indicating the filename of the study. If None,
            the study is not saved into a file.

        **kwargs: dict
            Optuna study parameters

        Returns
        ------------------------------------------------------

        study: optuna.study.Study
            The obtuna object which stores the whole study. See Optuna&#39;s documentation for more details:
            https://optuna.readthedocs.io/en/stable/reference/generated/optuna.study.Study.html#optuna.study.Study
        &#34;&#34;&#34;

        n_classes = len(np.unique(data[1]))
        self._n_classes = n_classes
        if self.training_columns is None:
            self.training_columns = list(data[0].columns)

        x_train, y_train, _, _ = data

        def __get_int_or_uniform(hyperparams_ranges, trial):

            params = {}

            for key in hyperparams_ranges:
                if isinstance(hyperparams_ranges[key][0], int):
                    params[key] = trial.suggest_int(
                        key, hyperparams_ranges[key][0], hyperparams_ranges[key][1])
                elif isinstance(hyperparams_ranges[key][0], float):
                    params[key] = trial.suggest_uniform(
                        key, hyperparams_ranges[key][0], hyperparams_ranges[key][1])

            return params

        def __objective(trial):

            params = __get_int_or_uniform(hyperparams_ranges, trial)
            model_copy = deepcopy(self.model)
            model_copy.set_params(**{**self.model_params, **params})
            return np.mean(cross_val_score(model_copy, x_train[self.training_columns], y_train,
                                           cv=nfold, scoring=cross_val_scoring, n_jobs=1))
        if resume_study:
            with open(resume_study, &#39;rb&#39;) as resume_study_file:
                study = pickle.load(resume_study_file)
        else:
            study = optuna.create_study(
                direction=direction, sampler=optuna_sampler)

        study.optimize(__objective, **kwargs)

        if save_study:
            with open(save_study, &#39;wb&#39;) as study_file:
                pickle.dump(study, study_file)

        print(f&#34;Number of finished trials: {len(study.trials)}&#34;)
        print(&#34;Best trial:&#34;)
        best_trial = study.best_trial

        print(f&#34;Value: {best_trial.value}&#34;)
        print(&#34;Params: &#34;)
        for key, value in best_trial.params.items():
            print(f&#34;    {key}: {value}&#34;)

        self.set_model_params({**self.model_params, **best_trial.params})

        return study

    def dump_original_model(self, filename, xgb_format=False):
        &#34;&#34;&#34;
        Save the trained model into a pickle
        file. Only for xgboost models it is also given
        the possibility to save them into a .model file

        Parameters
        -----------------------------------------------------
        filename: str
            Name of the file in which the model is saved

        xgb_format : bool
            If True saves the xgboost model into a .model file
        &#34;&#34;&#34;
        if xgb_format is False:
            with open(filename, &#34;wb&#34;) as output_file:
                pickle.dump(self.model, output_file)
        else:
            if self.model_string == &#39;xgboost&#39;:
                self.model.save_model(filename)
            else:
                print(&#34;File not saved: only xgboost models support the .model extension&#34;)

    def dump_model_handler(self, filename):
        &#34;&#34;&#34;
        Save the model handler into a pickle file

        Parameters
        -----------------------------------------------------
        filename: str
            Name of the file in which the model is saved
        &#34;&#34;&#34;
        with open(filename, &#34;wb&#34;) as output_file:
            pickle.dump(self, output_file)

    def load_model_handler(self, filename):
        &#34;&#34;&#34;
        Load a model handler saved into a pickle file

        Parameters
        -----------------------------------------------------
        filename: str
            Name of the file in which the model is saved
        &#34;&#34;&#34;
        with open(filename, &#34;rb&#34;) as input_file:
            loaded_model = pickle.load(input_file)
            self.model = loaded_model.get_original_model()
            self.training_columns = loaded_model.get_training_columns()
            self.model_params = loaded_model.get_model_params()
            self.model.set_params(**self.model_params)
            self.model_string = loaded_model.get_model_module()
            self._n_classes = loaded_model.get_n_classes()
            self._task_type = loaded_model.get_task_type()</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="hipe4ml.model_handler.ModelHandler"><code class="flex name class">
<span>class <span class="ident">ModelHandler</span></span>
<span>(</span><span>input_model=None, training_columns=None, model_params=None, task_type='classification')</span>
</code></dt>
<dd>
<div class="desc"><p>Class used for wrapping the models from different ML libraries to
build a new model with common methods. Currently LightGBM, XGBoost
(through their sklearn interface) and sklearn models are supported.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>input_model</code></strong> :&ensp;<code>XGBoost, LightGBM</code> or <code>sklearn model</code></dt>
<dd>&nbsp;</dd>
<dt><strong><code>training_columns</code></strong> :&ensp;<code>list</code></dt>
<dd>Contains the name of the features used for the training.
Example: ['dEdx', 'pT', 'ct']</dd>
<dt><strong><code>model_params</code></strong> :&ensp;<code>dict</code></dt>
<dd>Model hyper-parameter values. For
example (XGBoost): max_depth, learning_rate,
n_estimators, gamma, min_child_weight, &hellip;</dd>
<dt><strong><code>task_type</code></strong> :&ensp;<code>str</code></dt>
<dd>Task type of the model: 'classification' or 'regression'</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class ModelHandler:
    &#34;&#34;&#34;
    Class used for wrapping the models from different ML libraries to
    build a new model with common methods. Currently LightGBM, XGBoost
    (through their sklearn interface) and sklearn models are supported.

    Parameters
    -------------------------------------------------
    input_model: XGBoost, LightGBM or sklearn model

    training_columns: list
        Contains the name of the features used for the training.
        Example: [&#39;dEdx&#39;, &#39;pT&#39;, &#39;ct&#39;]

    model_params: dict
        Model hyper-parameter values. For
        example (XGBoost): max_depth, learning_rate,
        n_estimators, gamma, min_child_weight, ...

    task_type: str
        Task type of the model: &#39;classification&#39; or &#39;regression&#39;
    &#34;&#34;&#34;

    def __init__(self, input_model=None, training_columns=None, model_params=None, task_type=&#39;classification&#39;):
        self.model = input_model
        self.training_columns = training_columns
        self.model_params = model_params
        self._n_classes = None
        self._task_type = task_type
        if self._task_type not in [&#39;classification&#39;, &#39;regression&#39;]:
            raise ValueError(
                &#34;Task type must be either &#39;classification&#39; or &#39;regression&#39;&#34;)

        if self.model is not None:
            self.model_string = inspect.getmodule(
                self.model).__name__.partition(&#39;.&#39;)[0]

            if self.model_string not in [&#34;xgboost&#34;, &#34;lightgbm&#34;, &#34;sklearn&#34;]:
                raise ValueError(
                    &#34;Model must be either &#39;xgboost&#39;, &#39;lightgbm&#39; or &#39;sklearn&#39;&#34;)

            if self.model_params is None:
                self.model_params = self.model.get_params()
            else:
                self.model.set_params(**self.model_params)

    def set_model_params(self, model_params):
        &#34;&#34;&#34;
        Set the model (hyper-)parameters

        Parameters
        ------------------------------------
        model_params: dict
            Model hyper-parameter values. For
            example (XGBoost): max_depth, learning_rate,
            n_estimators, gamma, min_child_weight, ...
        &#34;&#34;&#34;
        self.model_params = model_params
        self.model.set_params(**self.model_params)

    def get_model_params(self):
        &#34;&#34;&#34;
        Get the model (hyper-)parameters

        Returns
        ------------------------------------
        out: dict
            Model hyper-parameter values. For
            example (XGBoost): max_depth, learning_rate,
            n_estimators, gamma, min_child_weight, ...
        &#34;&#34;&#34;
        return self.model.get_params()

    def set_training_columns(self, training_columns):
        &#34;&#34;&#34;
        Set the features used for the training process

        Parameters
        ------------------------------------
        training_columns: list
            Contains the name of the features used for the training.
            Example: [&#39;dEdx&#39;, &#39;pT&#39;, &#39;ct&#39;]
        &#34;&#34;&#34;
        self.training_columns = training_columns

    def get_training_columns(self):
        &#34;&#34;&#34;
        Get the features used for the training process

        Returns
        ------------------------------------
        out: list
            Names of the features used for the training.
            Example: [&#39;dEdx&#39;, &#39;pT&#39;, &#39;ct&#39;]
        &#34;&#34;&#34;

        return self.training_columns

    def get_original_model(self):
        &#34;&#34;&#34;
        Get the original unwrapped model

        Returns
        ---------------------------
        out: XGBoost, LGBM or sklearn model
        &#34;&#34;&#34;
        return self.model

    def get_model_module(self):
        &#34;&#34;&#34;
        Get the string containing the name
        of the model module

        Returns
        ---------------------------
        out: str
            Name of the model module
        &#34;&#34;&#34;
        return self.model_string

    def get_n_classes(self):
        &#34;&#34;&#34;
        Get the number of classes

        Returns
        ---------------------------
        out: int
            Number of classes
        &#34;&#34;&#34;
        return self._n_classes

    def get_task_type(self):
        &#34;&#34;&#34;
        Get the task type of the model

        Returns
        ---------------------------
        out: str
            Task type of the model: &#39;classification&#39; or &#39;regression&#39;
        &#34;&#34;&#34;
        return self._task_type

    def fit(self, x_train, y_train, **kwargs):
        &#34;&#34;&#34;
        Fit Model

        Parameters
        ---------------------------
        x_train: array-like, sparse matrix
            Training data

        y_train: array-like, sparse matrix
            Target data

        **kwargs:
            Extra kwargs passed on to model.fit() method
        &#34;&#34;&#34;
        if self._task_type == &#39;classification&#39;:
            n_classes = len(np.unique(y_train))
            self._n_classes = n_classes
        if self.training_columns is None:
            self.training_columns = list(x_train.columns)

        self.model.fit(x_train[self.training_columns], y_train, **kwargs)

    def predict(self, x_test, output_margin=True, **kwargs):
        &#34;&#34;&#34;
        Return model prediction for the array x_test
        Parameters
        --------------------------------------
        x_test: hipe4ml tree_handler, array-like, sparse matrix
            The input sample.

        output_margin: bool
            Whether to output the raw untransformed margin value. If False model
            probabilities are returned. Not used when task_type is &#39;regression&#39;.

        **kwargs:
            Extra kwargs passed on to the following model prediction function:
            if (task_type == &#39;classification&#39;)
            - predict() (XGBoost and LGBM) or decision_function() (sklearn) if output_margin==True
            - predict_proba() if output_margin==False
            if (task_type == &#39;regression&#39;)
            - predict()

        Returns
        ---------------------------------------
        out: numpy array
            Model predictions
        &#34;&#34;&#34;
        if isinstance(x_test, hipe4ml.tree_handler.TreeHandler):
            x_test = x_test.get_data_frame()

        x_test = x_test[self.training_columns]

        # regression
        if self._task_type == &#39;regression&#39;:
            return self.model.predict(x_test, **kwargs)

        # classification
        if output_margin:
            if self.model_string == &#39;xgboost&#39;:
                return self.model.predict(x_test, output_margin=True, **kwargs)
            if self.model_string == &#39;lightgbm&#39;:
                return self.model.predict(x_test, raw_score=True, **kwargs)
            if self.model_string == &#39;sklearn&#39;:
                if not hasattr(self.model, &#39;decision_function&#39;):
                    raise ValueError(
                        &#34;This Model does not support a decision_function(): use output_margin=False&#34;)
                return self.model.decision_function(x_test, **kwargs).ravel()

        pred = self.model.predict_proba(x_test, **kwargs)
        # in case of binary classification return only the scores of
        # the signal class
        if pred.shape[1] &lt;= 2:
            pred = pred[:, 1]
        return pred

    def train_test_model(self, data, return_prediction=False, output_margin=False, average=&#39;macro&#39;,
                         multi_class_opt=&#39;raise&#39;, **kwargs):
        &#34;&#34;&#34;
        Perform the training and the testing of the model. The model performance is estimated
        using the ROC AUC metric for classification and the MSE for regression.

        Parameters
        ----------------------------------------------
        data: list
            Contains respectively: training
            set dataframe, training label array,
            test set dataframe, test label array

        return_prediction: bool
            If True Model predictions on the test set are
            returned

        output_margin: bool
            Whether to output the raw untransformed margin value. If False model
            probabilities are returned. Not used when task_type is &#39;regression&#39;.

        average: string
            Option for the average of ROC AUC scores used only in case of multi-classification.
            You can choose between &#39;macro&#39; and &#39;weighted&#39;. For more information see
            https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html#sklearn.metrics.roc_auc_score

        multi_class_opt: string
            Option to compute ROC AUC scores used only in case of multi-classification.
            The one-vs-one &#39;ovo&#39; and one-vs-rest &#39;ovr&#39; approaches are available

        **kwargs: dict
            Extra kwargs passed on to the model fit method

        Returns
        ---------------------------------------
        out: numpy array or None
            If return_prediction==True, Model predictions on the test set are
            returned

        &#34;&#34;&#34;

        # get number of classes
        n_classes = len(np.unique(data[1]))
        self._n_classes = n_classes
        print(&#39;==============================&#39;)
        print(f&#34;Training {self.model_string} model for {self._task_type}&#34;)
        if self._task_type == &#39;classification&#39;:
            print(&#39;Number of detected classes:&#39;, n_classes)

        # final training with the optimized hyperparams
        print(&#39;Training the model: ...&#39;)
        self.fit(data[0], data[1], **kwargs)
        print(&#39;Training the model: Done!&#39;)
        print(&#39;Testing the model: ...&#39;)
        y_pred = self.predict(data[2], output_margin=output_margin)
        if self._task_type == &#39;classification&#39;:
            roc_score = roc_auc_score(
                data[3], y_pred, average=average, multi_class=multi_class_opt)
            print(f&#39;ROC_AUC_score: {roc_score:.6f}&#39;)
        else:
            mse_score = mean_squared_error(data[3], y_pred)
            print(f&#39;Mean squared error: {mse_score:.6f}&#39;)
        print(&#39;Testing the model: Done!&#39;)
        print(&#39;==============================&#39;)
        if return_prediction:
            return y_pred
        return None

    def optimize_params_optuna(self, data, hyperparams_ranges, cross_val_scoring, nfold=5, direction=&#39;maximize&#39;,
                               optuna_sampler=None, resume_study=None, save_study=None, **kwargs):
        &#34;&#34;&#34;
        Perform hyperparameter optimization of ModelHandler using the Optuna module.
        The model hyperparameters are automatically set as the ones that provided the
        best result during the optimization.

        Parameters
        ------------------------------------------------------
        data: list
            Contains respectively: training
            set dataframe, training label array,
            test set dataframe, test label array

        hyperparams_ranges: dict
            Hyperparameter ranges (in tuples or list). If a parameter is not
            in a tuple or a list it will be considered constant.
            Important: the type of the params must be preserved
            when passing the ranges.
            For example:
            dict={
                &#39;max_depth&#39;:(10,100)
                &#39;learning_rate&#39;: (0.01,0.03)
                &#39;n_jobs&#39;: 8
            }

        cross_val_scoring: string, callable or None
            Score metrics used for the cross-validation.
            A string (see sklearn model evaluation documentation:
            https://scikit-learn.org/stable/modules/model_evaluation.html)
            or a scorer callable object / function with signature scorer(estimator, X, y)
            which should return only a single value.
            In binary classification &#39;roc_auc&#39; is suggested.
            In multi-classification one between ‘roc_auc_ovr’, ‘roc_auc_ovo’,
            ‘roc_auc_ovr_weighted’ and ‘roc_auc_ovo_weighted’ is suggested.
            For more information see
            https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter

        direction: str
            The direction of optimization. Either &#39;maximize&#39; or &#39;minimize&#39;.
            (e.g. for the metric &#39;roc_auc&#39; the direction is &#39;maximize&#39;)

        optuna_sampler: optuna.samplers.BaseSampler
            Sampler to be used for the optuna (maxi-)minimisation.
            If None, default TPESampler is used. For more information see:
            https://optuna.readthedocs.io/en/stable/reference/samplers.html

        nfold: int
            Number of folds to calculate the cross validation error

        resume_study: str
            A string indicating the filename of the study to be resumed.
            If None, the study is not resumed.

        save_study: str
            A string indicating the filename of the study. If None,
            the study is not saved into a file.

        **kwargs: dict
            Optuna study parameters

        Returns
        ------------------------------------------------------

        study: optuna.study.Study
            The obtuna object which stores the whole study. See Optuna&#39;s documentation for more details:
            https://optuna.readthedocs.io/en/stable/reference/generated/optuna.study.Study.html#optuna.study.Study
        &#34;&#34;&#34;

        n_classes = len(np.unique(data[1]))
        self._n_classes = n_classes
        if self.training_columns is None:
            self.training_columns = list(data[0].columns)

        x_train, y_train, _, _ = data

        def __get_int_or_uniform(hyperparams_ranges, trial):

            params = {}

            for key in hyperparams_ranges:
                if isinstance(hyperparams_ranges[key][0], int):
                    params[key] = trial.suggest_int(
                        key, hyperparams_ranges[key][0], hyperparams_ranges[key][1])
                elif isinstance(hyperparams_ranges[key][0], float):
                    params[key] = trial.suggest_uniform(
                        key, hyperparams_ranges[key][0], hyperparams_ranges[key][1])

            return params

        def __objective(trial):

            params = __get_int_or_uniform(hyperparams_ranges, trial)
            model_copy = deepcopy(self.model)
            model_copy.set_params(**{**self.model_params, **params})
            return np.mean(cross_val_score(model_copy, x_train[self.training_columns], y_train,
                                           cv=nfold, scoring=cross_val_scoring, n_jobs=1))
        if resume_study:
            with open(resume_study, &#39;rb&#39;) as resume_study_file:
                study = pickle.load(resume_study_file)
        else:
            study = optuna.create_study(
                direction=direction, sampler=optuna_sampler)

        study.optimize(__objective, **kwargs)

        if save_study:
            with open(save_study, &#39;wb&#39;) as study_file:
                pickle.dump(study, study_file)

        print(f&#34;Number of finished trials: {len(study.trials)}&#34;)
        print(&#34;Best trial:&#34;)
        best_trial = study.best_trial

        print(f&#34;Value: {best_trial.value}&#34;)
        print(&#34;Params: &#34;)
        for key, value in best_trial.params.items():
            print(f&#34;    {key}: {value}&#34;)

        self.set_model_params({**self.model_params, **best_trial.params})

        return study

    def dump_original_model(self, filename, xgb_format=False):
        &#34;&#34;&#34;
        Save the trained model into a pickle
        file. Only for xgboost models it is also given
        the possibility to save them into a .model file

        Parameters
        -----------------------------------------------------
        filename: str
            Name of the file in which the model is saved

        xgb_format : bool
            If True saves the xgboost model into a .model file
        &#34;&#34;&#34;
        if xgb_format is False:
            with open(filename, &#34;wb&#34;) as output_file:
                pickle.dump(self.model, output_file)
        else:
            if self.model_string == &#39;xgboost&#39;:
                self.model.save_model(filename)
            else:
                print(&#34;File not saved: only xgboost models support the .model extension&#34;)

    def dump_model_handler(self, filename):
        &#34;&#34;&#34;
        Save the model handler into a pickle file

        Parameters
        -----------------------------------------------------
        filename: str
            Name of the file in which the model is saved
        &#34;&#34;&#34;
        with open(filename, &#34;wb&#34;) as output_file:
            pickle.dump(self, output_file)

    def load_model_handler(self, filename):
        &#34;&#34;&#34;
        Load a model handler saved into a pickle file

        Parameters
        -----------------------------------------------------
        filename: str
            Name of the file in which the model is saved
        &#34;&#34;&#34;
        with open(filename, &#34;rb&#34;) as input_file:
            loaded_model = pickle.load(input_file)
            self.model = loaded_model.get_original_model()
            self.training_columns = loaded_model.get_training_columns()
            self.model_params = loaded_model.get_model_params()
            self.model.set_params(**self.model_params)
            self.model_string = loaded_model.get_model_module()
            self._n_classes = loaded_model.get_n_classes()
            self._task_type = loaded_model.get_task_type()</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="hipe4ml.model_handler.ModelHandler.dump_model_handler"><code class="name flex">
<span>def <span class="ident">dump_model_handler</span></span>(<span>self, filename)</span>
</code></dt>
<dd>
<div class="desc"><p>Save the model handler into a pickle file</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>filename</code></strong> :&ensp;<code>str</code></dt>
<dd>Name of the file in which the model is saved</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def dump_model_handler(self, filename):
    &#34;&#34;&#34;
    Save the model handler into a pickle file

    Parameters
    -----------------------------------------------------
    filename: str
        Name of the file in which the model is saved
    &#34;&#34;&#34;
    with open(filename, &#34;wb&#34;) as output_file:
        pickle.dump(self, output_file)</code></pre>
</details>
</dd>
<dt id="hipe4ml.model_handler.ModelHandler.dump_original_model"><code class="name flex">
<span>def <span class="ident">dump_original_model</span></span>(<span>self, filename, xgb_format=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Save the trained model into a pickle
file. Only for xgboost models it is also given
the possibility to save them into a .model file</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>filename</code></strong> :&ensp;<code>str</code></dt>
<dd>Name of the file in which the model is saved</dd>
<dt><strong><code>xgb_format</code></strong> :&ensp;<code>bool</code></dt>
<dd>If True saves the xgboost model into a .model file</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def dump_original_model(self, filename, xgb_format=False):
    &#34;&#34;&#34;
    Save the trained model into a pickle
    file. Only for xgboost models it is also given
    the possibility to save them into a .model file

    Parameters
    -----------------------------------------------------
    filename: str
        Name of the file in which the model is saved

    xgb_format : bool
        If True saves the xgboost model into a .model file
    &#34;&#34;&#34;
    if xgb_format is False:
        with open(filename, &#34;wb&#34;) as output_file:
            pickle.dump(self.model, output_file)
    else:
        if self.model_string == &#39;xgboost&#39;:
            self.model.save_model(filename)
        else:
            print(&#34;File not saved: only xgboost models support the .model extension&#34;)</code></pre>
</details>
</dd>
<dt id="hipe4ml.model_handler.ModelHandler.fit"><code class="name flex">
<span>def <span class="ident">fit</span></span>(<span>self, x_train, y_train, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Fit Model</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>x_train</code></strong> :&ensp;<code>array-like, sparse matrix</code></dt>
<dd>Training data</dd>
<dt><strong><code>y_train</code></strong> :&ensp;<code>array-like, sparse matrix</code></dt>
<dd>Target data</dd>
</dl>
<p>**kwargs:
Extra kwargs passed on to model.fit() method</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def fit(self, x_train, y_train, **kwargs):
    &#34;&#34;&#34;
    Fit Model

    Parameters
    ---------------------------
    x_train: array-like, sparse matrix
        Training data

    y_train: array-like, sparse matrix
        Target data

    **kwargs:
        Extra kwargs passed on to model.fit() method
    &#34;&#34;&#34;
    if self._task_type == &#39;classification&#39;:
        n_classes = len(np.unique(y_train))
        self._n_classes = n_classes
    if self.training_columns is None:
        self.training_columns = list(x_train.columns)

    self.model.fit(x_train[self.training_columns], y_train, **kwargs)</code></pre>
</details>
</dd>
<dt id="hipe4ml.model_handler.ModelHandler.get_model_module"><code class="name flex">
<span>def <span class="ident">get_model_module</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Get the string containing the name
of the model module</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>out</code></strong> :&ensp;<code>str</code></dt>
<dd>Name of the model module</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_model_module(self):
    &#34;&#34;&#34;
    Get the string containing the name
    of the model module

    Returns
    ---------------------------
    out: str
        Name of the model module
    &#34;&#34;&#34;
    return self.model_string</code></pre>
</details>
</dd>
<dt id="hipe4ml.model_handler.ModelHandler.get_model_params"><code class="name flex">
<span>def <span class="ident">get_model_params</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Get the model (hyper-)parameters</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>out</code></strong> :&ensp;<code>dict</code></dt>
<dd>Model hyper-parameter values. For
example (XGBoost): max_depth, learning_rate,
n_estimators, gamma, min_child_weight, &hellip;</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_model_params(self):
    &#34;&#34;&#34;
    Get the model (hyper-)parameters

    Returns
    ------------------------------------
    out: dict
        Model hyper-parameter values. For
        example (XGBoost): max_depth, learning_rate,
        n_estimators, gamma, min_child_weight, ...
    &#34;&#34;&#34;
    return self.model.get_params()</code></pre>
</details>
</dd>
<dt id="hipe4ml.model_handler.ModelHandler.get_n_classes"><code class="name flex">
<span>def <span class="ident">get_n_classes</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Get the number of classes</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>out</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of classes</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_n_classes(self):
    &#34;&#34;&#34;
    Get the number of classes

    Returns
    ---------------------------
    out: int
        Number of classes
    &#34;&#34;&#34;
    return self._n_classes</code></pre>
</details>
</dd>
<dt id="hipe4ml.model_handler.ModelHandler.get_original_model"><code class="name flex">
<span>def <span class="ident">get_original_model</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Get the original unwrapped model</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>out</code></strong> :&ensp;<code>XGBoost, LGBM</code> or <code>sklearn model</code></dt>
<dd>&nbsp;</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_original_model(self):
    &#34;&#34;&#34;
    Get the original unwrapped model

    Returns
    ---------------------------
    out: XGBoost, LGBM or sklearn model
    &#34;&#34;&#34;
    return self.model</code></pre>
</details>
</dd>
<dt id="hipe4ml.model_handler.ModelHandler.get_task_type"><code class="name flex">
<span>def <span class="ident">get_task_type</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Get the task type of the model</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>out</code></strong> :&ensp;<code>str</code></dt>
<dd>Task type of the model: 'classification' or 'regression'</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_task_type(self):
    &#34;&#34;&#34;
    Get the task type of the model

    Returns
    ---------------------------
    out: str
        Task type of the model: &#39;classification&#39; or &#39;regression&#39;
    &#34;&#34;&#34;
    return self._task_type</code></pre>
</details>
</dd>
<dt id="hipe4ml.model_handler.ModelHandler.get_training_columns"><code class="name flex">
<span>def <span class="ident">get_training_columns</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Get the features used for the training process</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>out</code></strong> :&ensp;<code>list</code></dt>
<dd>Names of the features used for the training.
Example: ['dEdx', 'pT', 'ct']</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_training_columns(self):
    &#34;&#34;&#34;
    Get the features used for the training process

    Returns
    ------------------------------------
    out: list
        Names of the features used for the training.
        Example: [&#39;dEdx&#39;, &#39;pT&#39;, &#39;ct&#39;]
    &#34;&#34;&#34;

    return self.training_columns</code></pre>
</details>
</dd>
<dt id="hipe4ml.model_handler.ModelHandler.load_model_handler"><code class="name flex">
<span>def <span class="ident">load_model_handler</span></span>(<span>self, filename)</span>
</code></dt>
<dd>
<div class="desc"><p>Load a model handler saved into a pickle file</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>filename</code></strong> :&ensp;<code>str</code></dt>
<dd>Name of the file in which the model is saved</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def load_model_handler(self, filename):
    &#34;&#34;&#34;
    Load a model handler saved into a pickle file

    Parameters
    -----------------------------------------------------
    filename: str
        Name of the file in which the model is saved
    &#34;&#34;&#34;
    with open(filename, &#34;rb&#34;) as input_file:
        loaded_model = pickle.load(input_file)
        self.model = loaded_model.get_original_model()
        self.training_columns = loaded_model.get_training_columns()
        self.model_params = loaded_model.get_model_params()
        self.model.set_params(**self.model_params)
        self.model_string = loaded_model.get_model_module()
        self._n_classes = loaded_model.get_n_classes()
        self._task_type = loaded_model.get_task_type()</code></pre>
</details>
</dd>
<dt id="hipe4ml.model_handler.ModelHandler.optimize_params_optuna"><code class="name flex">
<span>def <span class="ident">optimize_params_optuna</span></span>(<span>self, data, hyperparams_ranges, cross_val_scoring, nfold=5, direction='maximize', optuna_sampler=None, resume_study=None, save_study=None, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Perform hyperparameter optimization of ModelHandler using the Optuna module.
The model hyperparameters are automatically set as the ones that provided the
best result during the optimization.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>data</code></strong> :&ensp;<code>list</code></dt>
<dd>Contains respectively: training
set dataframe, training label array,
test set dataframe, test label array</dd>
<dt><strong><code>hyperparams_ranges</code></strong> :&ensp;<code>dict</code></dt>
<dd>Hyperparameter ranges (in tuples or list). If a parameter is not
in a tuple or a list it will be considered constant.
Important: the type of the params must be preserved
when passing the ranges.
For example:
dict={
'max_depth':(10,100)
'learning_rate': (0.01,0.03)
'n_jobs': 8
}</dd>
<dt><strong><code>cross_val_scoring</code></strong> :&ensp;<code>string, callable</code> or <code>None</code></dt>
<dd>Score metrics used for the cross-validation.
A string (see sklearn model evaluation documentation:
<a href="https://scikit-learn.org/stable/modules/model_evaluation.html">https://scikit-learn.org/stable/modules/model_evaluation.html</a>)
or a scorer callable object / function with signature scorer(estimator, X, y)
which should return only a single value.
In binary classification 'roc_auc' is suggested.
In multi-classification one between ‘roc_auc_ovr’, ‘roc_auc_ovo’,
‘roc_auc_ovr_weighted’ and ‘roc_auc_ovo_weighted’ is suggested.
For more information see
<a href="https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter">https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter</a></dd>
<dt><strong><code>direction</code></strong> :&ensp;<code>str</code></dt>
<dd>The direction of optimization. Either 'maximize' or 'minimize'.
(e.g. for the metric 'roc_auc' the direction is 'maximize')</dd>
<dt><strong><code>optuna_sampler</code></strong> :&ensp;<code>optuna.samplers.BaseSampler</code></dt>
<dd>Sampler to be used for the optuna (maxi-)minimisation.
If None, default TPESampler is used. For more information see:
<a href="https://optuna.readthedocs.io/en/stable/reference/samplers.html">https://optuna.readthedocs.io/en/stable/reference/samplers.html</a></dd>
<dt><strong><code>nfold</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of folds to calculate the cross validation error</dd>
<dt><strong><code>resume_study</code></strong> :&ensp;<code>str</code></dt>
<dd>A string indicating the filename of the study to be resumed.
If None, the study is not resumed.</dd>
<dt><strong><code>save_study</code></strong> :&ensp;<code>str</code></dt>
<dd>A string indicating the filename of the study. If None,
the study is not saved into a file.</dd>
<dt><strong><code>**kwargs</code></strong> :&ensp;<code>dict</code></dt>
<dd>Optuna study parameters</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>study</code></strong> :&ensp;<code>optuna.study.Study</code></dt>
<dd>The obtuna object which stores the whole study. See Optuna's documentation for more details:
<a href="https://optuna.readthedocs.io/en/stable/reference/generated/optuna.study.Study.html#optuna.study.Study">https://optuna.readthedocs.io/en/stable/reference/generated/optuna.study.Study.html#optuna.study.Study</a></dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def optimize_params_optuna(self, data, hyperparams_ranges, cross_val_scoring, nfold=5, direction=&#39;maximize&#39;,
                           optuna_sampler=None, resume_study=None, save_study=None, **kwargs):
    &#34;&#34;&#34;
    Perform hyperparameter optimization of ModelHandler using the Optuna module.
    The model hyperparameters are automatically set as the ones that provided the
    best result during the optimization.

    Parameters
    ------------------------------------------------------
    data: list
        Contains respectively: training
        set dataframe, training label array,
        test set dataframe, test label array

    hyperparams_ranges: dict
        Hyperparameter ranges (in tuples or list). If a parameter is not
        in a tuple or a list it will be considered constant.
        Important: the type of the params must be preserved
        when passing the ranges.
        For example:
        dict={
            &#39;max_depth&#39;:(10,100)
            &#39;learning_rate&#39;: (0.01,0.03)
            &#39;n_jobs&#39;: 8
        }

    cross_val_scoring: string, callable or None
        Score metrics used for the cross-validation.
        A string (see sklearn model evaluation documentation:
        https://scikit-learn.org/stable/modules/model_evaluation.html)
        or a scorer callable object / function with signature scorer(estimator, X, y)
        which should return only a single value.
        In binary classification &#39;roc_auc&#39; is suggested.
        In multi-classification one between ‘roc_auc_ovr’, ‘roc_auc_ovo’,
        ‘roc_auc_ovr_weighted’ and ‘roc_auc_ovo_weighted’ is suggested.
        For more information see
        https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter

    direction: str
        The direction of optimization. Either &#39;maximize&#39; or &#39;minimize&#39;.
        (e.g. for the metric &#39;roc_auc&#39; the direction is &#39;maximize&#39;)

    optuna_sampler: optuna.samplers.BaseSampler
        Sampler to be used for the optuna (maxi-)minimisation.
        If None, default TPESampler is used. For more information see:
        https://optuna.readthedocs.io/en/stable/reference/samplers.html

    nfold: int
        Number of folds to calculate the cross validation error

    resume_study: str
        A string indicating the filename of the study to be resumed.
        If None, the study is not resumed.

    save_study: str
        A string indicating the filename of the study. If None,
        the study is not saved into a file.

    **kwargs: dict
        Optuna study parameters

    Returns
    ------------------------------------------------------

    study: optuna.study.Study
        The obtuna object which stores the whole study. See Optuna&#39;s documentation for more details:
        https://optuna.readthedocs.io/en/stable/reference/generated/optuna.study.Study.html#optuna.study.Study
    &#34;&#34;&#34;

    n_classes = len(np.unique(data[1]))
    self._n_classes = n_classes
    if self.training_columns is None:
        self.training_columns = list(data[0].columns)

    x_train, y_train, _, _ = data

    def __get_int_or_uniform(hyperparams_ranges, trial):

        params = {}

        for key in hyperparams_ranges:
            if isinstance(hyperparams_ranges[key][0], int):
                params[key] = trial.suggest_int(
                    key, hyperparams_ranges[key][0], hyperparams_ranges[key][1])
            elif isinstance(hyperparams_ranges[key][0], float):
                params[key] = trial.suggest_uniform(
                    key, hyperparams_ranges[key][0], hyperparams_ranges[key][1])

        return params

    def __objective(trial):

        params = __get_int_or_uniform(hyperparams_ranges, trial)
        model_copy = deepcopy(self.model)
        model_copy.set_params(**{**self.model_params, **params})
        return np.mean(cross_val_score(model_copy, x_train[self.training_columns], y_train,
                                       cv=nfold, scoring=cross_val_scoring, n_jobs=1))
    if resume_study:
        with open(resume_study, &#39;rb&#39;) as resume_study_file:
            study = pickle.load(resume_study_file)
    else:
        study = optuna.create_study(
            direction=direction, sampler=optuna_sampler)

    study.optimize(__objective, **kwargs)

    if save_study:
        with open(save_study, &#39;wb&#39;) as study_file:
            pickle.dump(study, study_file)

    print(f&#34;Number of finished trials: {len(study.trials)}&#34;)
    print(&#34;Best trial:&#34;)
    best_trial = study.best_trial

    print(f&#34;Value: {best_trial.value}&#34;)
    print(&#34;Params: &#34;)
    for key, value in best_trial.params.items():
        print(f&#34;    {key}: {value}&#34;)

    self.set_model_params({**self.model_params, **best_trial.params})

    return study</code></pre>
</details>
</dd>
<dt id="hipe4ml.model_handler.ModelHandler.predict"><code class="name flex">
<span>def <span class="ident">predict</span></span>(<span>self, x_test, output_margin=True, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Return model prediction for the array x_test
Parameters</p>
<hr>
<dl>
<dt><strong><code>x_test</code></strong> :&ensp;<code>hipe4ml tree_handler, array-like, sparse matrix</code></dt>
<dd>The input sample.</dd>
<dt><strong><code>output_margin</code></strong> :&ensp;<code>bool</code></dt>
<dd>Whether to output the raw untransformed margin value. If False model
probabilities are returned. Not used when task_type is 'regression'.</dd>
</dl>
<p>**kwargs:
Extra kwargs passed on to the following model prediction function:
if (task_type == 'classification')
- predict() (XGBoost and LGBM) or decision_function() (sklearn) if output_margin==True
- predict_proba() if output_margin==False
if (task_type == 'regression')
- predict()</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>out</code></strong> :&ensp;<code>numpy array</code></dt>
<dd>Model predictions</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def predict(self, x_test, output_margin=True, **kwargs):
    &#34;&#34;&#34;
    Return model prediction for the array x_test
    Parameters
    --------------------------------------
    x_test: hipe4ml tree_handler, array-like, sparse matrix
        The input sample.

    output_margin: bool
        Whether to output the raw untransformed margin value. If False model
        probabilities are returned. Not used when task_type is &#39;regression&#39;.

    **kwargs:
        Extra kwargs passed on to the following model prediction function:
        if (task_type == &#39;classification&#39;)
        - predict() (XGBoost and LGBM) or decision_function() (sklearn) if output_margin==True
        - predict_proba() if output_margin==False
        if (task_type == &#39;regression&#39;)
        - predict()

    Returns
    ---------------------------------------
    out: numpy array
        Model predictions
    &#34;&#34;&#34;
    if isinstance(x_test, hipe4ml.tree_handler.TreeHandler):
        x_test = x_test.get_data_frame()

    x_test = x_test[self.training_columns]

    # regression
    if self._task_type == &#39;regression&#39;:
        return self.model.predict(x_test, **kwargs)

    # classification
    if output_margin:
        if self.model_string == &#39;xgboost&#39;:
            return self.model.predict(x_test, output_margin=True, **kwargs)
        if self.model_string == &#39;lightgbm&#39;:
            return self.model.predict(x_test, raw_score=True, **kwargs)
        if self.model_string == &#39;sklearn&#39;:
            if not hasattr(self.model, &#39;decision_function&#39;):
                raise ValueError(
                    &#34;This Model does not support a decision_function(): use output_margin=False&#34;)
            return self.model.decision_function(x_test, **kwargs).ravel()

    pred = self.model.predict_proba(x_test, **kwargs)
    # in case of binary classification return only the scores of
    # the signal class
    if pred.shape[1] &lt;= 2:
        pred = pred[:, 1]
    return pred</code></pre>
</details>
</dd>
<dt id="hipe4ml.model_handler.ModelHandler.set_model_params"><code class="name flex">
<span>def <span class="ident">set_model_params</span></span>(<span>self, model_params)</span>
</code></dt>
<dd>
<div class="desc"><p>Set the model (hyper-)parameters</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>model_params</code></strong> :&ensp;<code>dict</code></dt>
<dd>Model hyper-parameter values. For
example (XGBoost): max_depth, learning_rate,
n_estimators, gamma, min_child_weight, &hellip;</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def set_model_params(self, model_params):
    &#34;&#34;&#34;
    Set the model (hyper-)parameters

    Parameters
    ------------------------------------
    model_params: dict
        Model hyper-parameter values. For
        example (XGBoost): max_depth, learning_rate,
        n_estimators, gamma, min_child_weight, ...
    &#34;&#34;&#34;
    self.model_params = model_params
    self.model.set_params(**self.model_params)</code></pre>
</details>
</dd>
<dt id="hipe4ml.model_handler.ModelHandler.set_training_columns"><code class="name flex">
<span>def <span class="ident">set_training_columns</span></span>(<span>self, training_columns)</span>
</code></dt>
<dd>
<div class="desc"><p>Set the features used for the training process</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>training_columns</code></strong> :&ensp;<code>list</code></dt>
<dd>Contains the name of the features used for the training.
Example: ['dEdx', 'pT', 'ct']</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def set_training_columns(self, training_columns):
    &#34;&#34;&#34;
    Set the features used for the training process

    Parameters
    ------------------------------------
    training_columns: list
        Contains the name of the features used for the training.
        Example: [&#39;dEdx&#39;, &#39;pT&#39;, &#39;ct&#39;]
    &#34;&#34;&#34;
    self.training_columns = training_columns</code></pre>
</details>
</dd>
<dt id="hipe4ml.model_handler.ModelHandler.train_test_model"><code class="name flex">
<span>def <span class="ident">train_test_model</span></span>(<span>self, data, return_prediction=False, output_margin=False, average='macro', multi_class_opt='raise', **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Perform the training and the testing of the model. The model performance is estimated
using the ROC AUC metric for classification and the MSE for regression.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>data</code></strong> :&ensp;<code>list</code></dt>
<dd>Contains respectively: training
set dataframe, training label array,
test set dataframe, test label array</dd>
<dt><strong><code>return_prediction</code></strong> :&ensp;<code>bool</code></dt>
<dd>If True Model predictions on the test set are
returned</dd>
<dt><strong><code>output_margin</code></strong> :&ensp;<code>bool</code></dt>
<dd>Whether to output the raw untransformed margin value. If False model
probabilities are returned. Not used when task_type is 'regression'.</dd>
<dt><strong><code>average</code></strong> :&ensp;<code>string</code></dt>
<dd>Option for the average of ROC AUC scores used only in case of multi-classification.
You can choose between 'macro' and 'weighted'. For more information see
<a href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html#sklearn.metrics.roc_auc_score">https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html#sklearn.metrics.roc_auc_score</a></dd>
<dt><strong><code>multi_class_opt</code></strong> :&ensp;<code>string</code></dt>
<dd>Option to compute ROC AUC scores used only in case of multi-classification.
The one-vs-one 'ovo' and one-vs-rest 'ovr' approaches are available</dd>
<dt><strong><code>**kwargs</code></strong> :&ensp;<code>dict</code></dt>
<dd>Extra kwargs passed on to the model fit method</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>out</code></strong> :&ensp;<code>numpy array</code> or <code>None</code></dt>
<dd>If return_prediction==True, Model predictions on the test set are
returned</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def train_test_model(self, data, return_prediction=False, output_margin=False, average=&#39;macro&#39;,
                     multi_class_opt=&#39;raise&#39;, **kwargs):
    &#34;&#34;&#34;
    Perform the training and the testing of the model. The model performance is estimated
    using the ROC AUC metric for classification and the MSE for regression.

    Parameters
    ----------------------------------------------
    data: list
        Contains respectively: training
        set dataframe, training label array,
        test set dataframe, test label array

    return_prediction: bool
        If True Model predictions on the test set are
        returned

    output_margin: bool
        Whether to output the raw untransformed margin value. If False model
        probabilities are returned. Not used when task_type is &#39;regression&#39;.

    average: string
        Option for the average of ROC AUC scores used only in case of multi-classification.
        You can choose between &#39;macro&#39; and &#39;weighted&#39;. For more information see
        https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html#sklearn.metrics.roc_auc_score

    multi_class_opt: string
        Option to compute ROC AUC scores used only in case of multi-classification.
        The one-vs-one &#39;ovo&#39; and one-vs-rest &#39;ovr&#39; approaches are available

    **kwargs: dict
        Extra kwargs passed on to the model fit method

    Returns
    ---------------------------------------
    out: numpy array or None
        If return_prediction==True, Model predictions on the test set are
        returned

    &#34;&#34;&#34;

    # get number of classes
    n_classes = len(np.unique(data[1]))
    self._n_classes = n_classes
    print(&#39;==============================&#39;)
    print(f&#34;Training {self.model_string} model for {self._task_type}&#34;)
    if self._task_type == &#39;classification&#39;:
        print(&#39;Number of detected classes:&#39;, n_classes)

    # final training with the optimized hyperparams
    print(&#39;Training the model: ...&#39;)
    self.fit(data[0], data[1], **kwargs)
    print(&#39;Training the model: Done!&#39;)
    print(&#39;Testing the model: ...&#39;)
    y_pred = self.predict(data[2], output_margin=output_margin)
    if self._task_type == &#39;classification&#39;:
        roc_score = roc_auc_score(
            data[3], y_pred, average=average, multi_class=multi_class_opt)
        print(f&#39;ROC_AUC_score: {roc_score:.6f}&#39;)
    else:
        mse_score = mean_squared_error(data[3], y_pred)
        print(f&#39;Mean squared error: {mse_score:.6f}&#39;)
    print(&#39;Testing the model: Done!&#39;)
    print(&#39;==============================&#39;)
    if return_prediction:
        return y_pred
    return None</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="hipe4ml" href="index.html">hipe4ml</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="hipe4ml.model_handler.ModelHandler" href="#hipe4ml.model_handler.ModelHandler">ModelHandler</a></code></h4>
<ul class="">
<li><code><a title="hipe4ml.model_handler.ModelHandler.dump_model_handler" href="#hipe4ml.model_handler.ModelHandler.dump_model_handler">dump_model_handler</a></code></li>
<li><code><a title="hipe4ml.model_handler.ModelHandler.dump_original_model" href="#hipe4ml.model_handler.ModelHandler.dump_original_model">dump_original_model</a></code></li>
<li><code><a title="hipe4ml.model_handler.ModelHandler.fit" href="#hipe4ml.model_handler.ModelHandler.fit">fit</a></code></li>
<li><code><a title="hipe4ml.model_handler.ModelHandler.get_model_module" href="#hipe4ml.model_handler.ModelHandler.get_model_module">get_model_module</a></code></li>
<li><code><a title="hipe4ml.model_handler.ModelHandler.get_model_params" href="#hipe4ml.model_handler.ModelHandler.get_model_params">get_model_params</a></code></li>
<li><code><a title="hipe4ml.model_handler.ModelHandler.get_n_classes" href="#hipe4ml.model_handler.ModelHandler.get_n_classes">get_n_classes</a></code></li>
<li><code><a title="hipe4ml.model_handler.ModelHandler.get_original_model" href="#hipe4ml.model_handler.ModelHandler.get_original_model">get_original_model</a></code></li>
<li><code><a title="hipe4ml.model_handler.ModelHandler.get_task_type" href="#hipe4ml.model_handler.ModelHandler.get_task_type">get_task_type</a></code></li>
<li><code><a title="hipe4ml.model_handler.ModelHandler.get_training_columns" href="#hipe4ml.model_handler.ModelHandler.get_training_columns">get_training_columns</a></code></li>
<li><code><a title="hipe4ml.model_handler.ModelHandler.load_model_handler" href="#hipe4ml.model_handler.ModelHandler.load_model_handler">load_model_handler</a></code></li>
<li><code><a title="hipe4ml.model_handler.ModelHandler.optimize_params_optuna" href="#hipe4ml.model_handler.ModelHandler.optimize_params_optuna">optimize_params_optuna</a></code></li>
<li><code><a title="hipe4ml.model_handler.ModelHandler.predict" href="#hipe4ml.model_handler.ModelHandler.predict">predict</a></code></li>
<li><code><a title="hipe4ml.model_handler.ModelHandler.set_model_params" href="#hipe4ml.model_handler.ModelHandler.set_model_params">set_model_params</a></code></li>
<li><code><a title="hipe4ml.model_handler.ModelHandler.set_training_columns" href="#hipe4ml.model_handler.ModelHandler.set_training_columns">set_training_columns</a></code></li>
<li><code><a title="hipe4ml.model_handler.ModelHandler.train_test_model" href="#hipe4ml.model_handler.ModelHandler.train_test_model">train_test_model</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>