<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.7.2" />
<title>hipe4ml.plot_utils API documentation</title>
<meta name="description" content="Module containing the plot utils. Each function returns a matplotlib object" />
<link href='https://cdnjs.cloudflare.com/ajax/libs/normalize/8.0.0/normalize.min.css' rel='stylesheet'>
<link href='https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/8.0.0/sanitize.min.css' rel='stylesheet'>
<link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css" rel="stylesheet">
<style>.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{font-weight:bold}#index h4 + ul{margin-bottom:.6em}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>hipe4ml.plot_utils</code></h1>
</header>
<section id="section-intro">
<p>Module containing the plot utils. Each function returns a matplotlib object</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#34;&#34;&#34; Module containing the plot utils. Each function returns a matplotlib object
    &#34;&#34;&#34;
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from pandas.core.index import Index
import shap
from mpl_toolkits.axes_grid1 import ImageGrid
from sklearn.metrics import (auc, average_precision_score,
                             precision_recall_curve, roc_curve, mean_squared_error)
from sklearn.preprocessing import label_binarize


def plot_output_train_test(
        model, data, bins=80, raw=True, labels=None, **kwds):
    &#34;&#34;&#34;
    Plot the BDT output distributions for each class and output
    both for training and test set.

    Input
    ----------------------------------------
    model: xgboost or sklearn model

    data: list
    Contains respectively: training
    set dataframe, training label array,
    test set dataframe, test label array

    bins: int or sequence of scalars or str
    If bins is an int, it defines the number of equal-width
    bins in the given range (10, by default). If bins is a
    sequence, it defines a monotonically increasing array of
    bin edges, including the rightmost edge, allowing for
    non-uniform bin widths.
    If bins is a string, it defines the method used to
    calculate the optimal bin width, as defined by
    np.histogram_bin_edges:
    https://docs.scipy.org/doc/numpy/reference/generated
    /numpy.histogram_bin_edges.html#numpy.histogram_bin_edges

    raw: Bool
    If true enables the raw untransformed margin value

    labels: list
    Contains the labels to be displayed in the legend
    If None the labels are class1, class2, ..., classN

    **kwds: extra arguments are passed on to plt.hist()

    Output
    ----------------------------------------
    list of matplotlib objects with the BDT output
    distributions for each class


    &#34;&#34;&#34;
    class_labels = np.unique(data[1])
    n_classes = len(class_labels)

    prediction = []
    for xxx, yyy in ((data[0], data[1]), (data[2], data[3])):
        for class_lab in class_labels:
            prediction.append(model.predict(xxx[yyy == class_lab], output_margin=raw))

    low = min(np.min(d) for d in prediction)
    high = max(np.max(d) for d in prediction)
    low_high = (low, high)

    res = []
    scale = 1.
    # only one figure in case of binary classification
    if n_classes &lt;= 2:
        labels = [&#39;Signal&#39;, &#39;Background&#39;] if labels is None else labels
        colors = [&#39;b&#39;, &#39;r&#39;]
        res.append(plt.figure())
        for i_class in range(n_classes):
            plt.hist(prediction[i_class], color=colors[i_class], alpha=0.5, range=low_high, bins=bins,
                     histtype=&#39;stepfilled&#39;, label=&#39;{} pdf Training Set&#39;.format(labels[i_class]), **kwds)

            hist, bins = np.histogram(prediction[i_class+2], bins=bins, range=low_high, **kwds)
            if &#39;density&#39; in kwds and kwds[&#39;density&#39;]:
                err = np.sqrt(hist * len(prediction[i_class])) / len(prediction[i_class+2])
            else:
                scale = len(prediction[i_class]) / sum(hist)
                err = np.sqrt(hist) * scale
            center = (bins[:-1] + bins[1:]) / 2
            plt.errorbar(center, hist * scale, yerr=err, fmt=&#39;o&#39;,
                         c=colors[i_class], label=&#39;{} pdf Test Set&#39;.format(labels[i_class]))

        plt.xlabel(&#39;BDT output&#39;, fontsize=13, ha=&#39;right&#39;, position=(1, 20))
        plt.ylabel(r&#39;                                Counts (arb. units)&#39;, fontsize=13)
        plt.legend(frameon=False, fontsize=12)
    # n figures in case of multi-classification with n classes
    else:
        labels = [&#39;class{}&#39;.format(i_class) for i_class, _ in enumerate(class_labels)] if labels is None else labels
        cmap = plt.cm.get_cmap(&#39;tab10&#39;)
        for i_output, _ in enumerate(class_labels):
            res.append(plt.figure())
            for i_class, lab in enumerate(labels):
                plt.hist(prediction[i_class][:, i_output], alpha=0.5, range=low_high, bins=bins,
                         color=cmap(i_class), histtype=&#39;stepfilled&#39;,
                         label=&#39;{} pdf Training Set&#39;.format(lab), **kwds)

                hist, bins = np.histogram(
                    prediction[i_class+n_classes][:, i_output], bins=bins, range=low_high, **kwds)
                if &#39;density&#39; in kwds and kwds[&#39;density&#39;]:
                    err = np.sqrt(hist * len(prediction[i_class][:, i_output])) / len(
                        prediction[i_class+n_classes][:, i_output])
                else:
                    scale = len(prediction[i_class][:, i_output]) / sum(hist)
                    err = np.sqrt(hist) * scale
                center = (bins[:-1] + bins[1:]) / 2

                plt.errorbar(center, hist * scale, yerr=err, fmt=&#39;o&#39;,
                             c=cmap(i_class), label=&#39;{} pdf Test Set&#39;.format(lab))

            plt.xlabel(&#39;BDT output for {}&#39;.format(labels[i_output]), fontsize=13, ha=&#39;right&#39;,
                       position=(1, 20))
            plt.ylabel(r&#39;                                Counts (arb. units)&#39;, fontsize=13)
            plt.legend(frameon=False, fontsize=12)

    return res


def plot_distr(list_of_df, column=None, figsize=None, bins=50, log=False, labels=None):
    &#34;&#34;&#34;
    Build a DataFrame and create a dataset for each class

    Draw histogram of the DataFrame&#39;s series comparing the
    distribution of each class.

    Input
    -----------------------------------------
    list_of_df: list
    Contains a dataframe for each class

    column: list
    Contains the name of the features you want to plot
    Example: [&#39;dEdx&#39;, &#39;pT&#39;, &#39;ct&#39;]

    figsize: list
    The size in inches of the figure to create. Uses the value in matplotlib.rcParams by default.

    bins: int or sequence of scalars or str
    If bins is an int, it defines the number of equal-width
    bins in the given range (10, by default). If bins is a
    sequence, it defines a monotonically increasing array of
    bin edges, including the rightmost edge, allowing for
    non-uniform bin widths.

    log: Bool
    If True enable log scale plot

    labels: list
    Contains the labels to be displayed in the legend
    If None the labels are class1, class2, ..., classN

    Output
    -----------------------------------------
    array of matplotlib axes with the distributions
    of the features for each class

    &#34;&#34;&#34;

    if column is not None:
        if not isinstance(column, (list, np.ndarray, Index)):
            column = [column]
        for dfm in list_of_df:
            dfm = dfm[column]

    if figsize is None:
        figsize = [20, 15]

    if labels is None:
        labels = [&#39;class{}&#39;.format(i_class) for i_class, _ in enumerate(list_of_df)]

    for i_class, (dfm, lab) in enumerate(zip(list_of_df, labels)):
        if i_class == 0:
            axes = dfm.hist(column=column, alpha=0.5, bins=bins, figsize=figsize, label=lab,
                            density=True, grid=False, log=log)
            axes = axes.flatten()
            axes = axes[:len(column)]
        else:
            dfm.hist(ax=axes, column=column, alpha=0.5, bins=bins, figsize=figsize, label=lab,
                     density=True, grid=False, log=log)
    for axs in axes:
        axs.set_ylabel(&#39;Counts (arb. units)&#39;)
    axes[-1].legend(loc=&#39;best&#39;)
    return axes


def plot_corr(list_of_df, columns, labels=None, **kwds):
    &#34;&#34;&#34;
    Calculate pairwise correlation between features for
    each class (e.g. signal and background in case of binary
    classification)

    Input
    -----------------------------------------------
    list_of_df: list
    Contains dataframes for each class

    columns: list
    Contains the name of the features you want to plot
    Example: [&#39;dEdx&#39;, &#39;pT&#39;, &#39;ct&#39;]

    labels: list
    Contains the labels to be displayed in the legend
    If None the labels are class1, class2, ..., classN

    **kwds: extra arguments are passed on to DataFrame.corr()

    Output
    ------------------------------------------------
    list of matplotlib objects with the correlations
    between the features for each class

    &#34;&#34;&#34;

    corr_mat = []
    for dfm in list_of_df:
        dfm = dfm[columns]
        corr_mat.append(dfm.corr(**kwds))

    if labels is None:
        labels = []
        if len(corr_mat) &gt; 2:
            for i_mat, _ in enumerate(corr_mat):
                labels.append(&#39;class{}&#39;.format(i_mat))
        else:
            labels.append(&#39;Signal&#39;)
            labels.append(&#39;Background&#39;)

    fig = []
    for mat, lab in zip(corr_mat, labels):
        fig.append(plt.figure(figsize=(8, 7)))
        grid = ImageGrid(fig[-1], 111, axes_pad=0.15, nrows_ncols=(1, 1), share_all=True,
                         cbar_location=&#39;right&#39;, cbar_mode=&#39;single&#39;, cbar_size=&#39;7%&#39;, cbar_pad=0.15)

        opts = {&#39;cmap&#39;: plt.get_cmap(
            &#39;coolwarm&#39;), &#39;vmin&#39;: -1, &#39;vmax&#39;: +1, &#39;snap&#39;: True}

        axs = grid[0]
        heatmap = axs.pcolor(mat, **opts)
        axs.set_title(lab, fontsize=14, fontweight=&#39;bold&#39;)

        lab = mat.columns.values

        # shift location of ticks to center of the bins
        axs.set_xticks(np.arange(len(lab)), minor=False)
        axs.set_yticks(np.arange(len(lab)), minor=False)
        axs.set_xticklabels(lab, minor=False, ha=&#39;left&#39;, rotation=90, fontsize=10)
        axs.set_yticklabels(lab, minor=False, va=&#39;bottom&#39;, fontsize=10)
        axs.tick_params(axis=&#39;both&#39;, which=&#39;both&#39;, direction=&#34;in&#34;)

        for tick in axs.xaxis.get_minor_ticks():
            tick.tick1line.set_markersize(0)
            tick.tick2line.set_markersize(0)
            tick.label1.set_horizontalalignment(&#39;center&#39;)

        axs.cax.colorbar(heatmap)

    return fig


def plot_bdt_eff(threshold, eff_sig):
    &#34;&#34;&#34;
    Plot the BDT efficiency calculated with the function
    bdt_efficiency_array() in analysis_utils

    Input
    -----------------------------------
    threshold: array
    Score threshold array

    eff_sig: array
    bdt efficiency array

    Output
    -----------------------------------
    matplotlib object of the bdt efficiency as a
    function of the threshold score

    &#34;&#34;&#34;
    res = plt.figure()
    plt.plot(threshold, eff_sig, &#39;r.&#39;, label=&#39;Signal efficiency&#39;)
    plt.legend()
    plt.xlabel(&#39;BDT Score&#39;)
    plt.ylabel(&#39;Efficiency&#39;)
    plt.title(&#39;Efficiency vs Score&#39;)
    plt.grid()
    return res


def plot_roc(y_truth, y_score, labels=None, pos_label=None):
    &#34;&#34;&#34;
    Calculate and plot the roc curve

    Input
    -------------------------------------

    y_truth : array
    True labels for the belonging class. If labels are not
    {0, 1, ..., N}, then pos_label should be explicitly given.

    y_score : array
    Target scores, can either be probability estimates
    of the positive class, confidence values, or
    non-thresholded measure of decisions (as returned
    by “decision_function” on some classifiers).

    labels: list
    Contains the labels to be displayed in the legend
    If None the labels are class1, class2, ..., classN

    pos_label : int or str
    The label of the positive class. When pos_label=None,
    if y_true is in {0, 1, ..., N}, pos_label is set to 1,
    otherwise an error will be raised.



    Output
    -------------------------------------
    matplotlib object with the roc curves

    &#34;&#34;&#34;
    # get number of classes
    n_classes = len(np.unique(y_truth))

    if (labels is None and n_classes &gt; 2) or (labels and len(labels) != n_classes):
        labels = []
        for i_class in range(n_classes):
            labels.append(&#39;class{}&#39;.format(i_class))

    res = plt.figure()
    if n_classes &lt;= 2:
        fpr, tpr, _ = roc_curve(y_truth, y_score, pos_label=pos_label)
        roc_auc = auc(fpr, tpr)
        plt.plot(fpr, tpr, lw=1, label=&#39;ROC (area = %0.4f)&#39; % (roc_auc))
    else:
        cmap = plt.cm.get_cmap(&#39;tab10&#39;)
        fpr, tpr, roc_auc = (dict() for i_dict in range(3))
        # convert multi-class labels to multi-labels to obtain roc curves
        y_truth_multi = label_binarize(y_truth, classes=range(n_classes))
        for clas, lab in enumerate(labels):
            fpr[clas], tpr[clas], _ = roc_curve(y_truth_multi[:, clas], y_score[:, clas])
            roc_auc[clas] = auc(fpr[clas], tpr[clas])
            plt.plot(fpr[clas], tpr[clas], lw=1, c=cmap(clas),
                     label=&#39;{0} (AUC = {1:.4f})&#39;.format(lab, roc_auc[clas]))
        # compute also micro average
        fpr[&#39;micro&#39;], tpr[&#39;micro&#39;], _ = roc_curve(y_truth_multi.ravel(), y_score.ravel())
        roc_auc[&#39;micro&#39;] = auc(fpr[&#39;micro&#39;], tpr[&#39;micro&#39;])
        plt.plot(fpr[&#39;micro&#39;], tpr[&#39;micro&#39;], lw=1, linestyle=&#39;--&#39;, c=&#39;black&#39;,
                 label=&#39;average (AUC = {:.4f})&#39;.format(roc_auc[&#39;micro&#39;]))

    plt.plot([0, 1], [0, 1], &#39;-.&#39;, color=(0.6, 0.6, 0.6), label=&#39;Luck&#39;)
    plt.xlim([-0.05, 1.05])
    plt.ylim([-0.05, 1.05])
    plt.xlabel(&#39;False Positive Rate&#39;, fontsize=12)
    plt.ylabel(&#39;True Positive Rate&#39;, fontsize=12)
    plt.legend(loc=&#39;lower right&#39;)
    plt.grid()
    return res


def plot_roc_train_test(y_truth_test, y_score_test, y_truth_train, y_score_train, labels=None, pos_label=None):
    &#34;&#34;&#34;
    Calculate and plot the roc curve for test and train sets

    Input
    -------------------------------------

    y_truth_test : array
    True labels for the belonging class of the test set.
    If labels are not {0, 1, ..., N}, then pos_label
    should be explicitly given.

    y_score_test : array
    Target scores for the test set, can either be probability
    estimates of the positive class, confidence values, or
    non-thresholded measure of decisions (as returned
    by “decision_function” on some classifiers).

    y_truth_train : array
    True labels for the belonging class of the train set.
    If labels are not {0, 1, ..., N}, then pos_label
    should be explicitly given.

    y_score_train : array
    Target scores for the train set, can either be probability
    estimates of the positive class, confidence values, or
    non-thresholded measure of decisions (as returned
    by “decision_function” on some classifiers).

    labels: list
    Contains the labels to be displayed in the legend
    If None the labels are class1, class2, ..., classN

    pos_label : int or str
    The label of the positive class. When pos_label=None,
    if y_true_ is in {0, 1, ..., N}, pos_label is set to 1,
    otherwise an error will be raised.



    Output
    -------------------------------------
    matplotlib object with the roc curves

    &#34;&#34;&#34;
    # get number of classes
    n_classes = len(np.unique(y_truth_test))

    if (labels is None and n_classes &gt; 2) or (labels and len(labels) != n_classes):
        labels = []
        for i_class in range(n_classes):
            labels.append(&#39;class{}&#39;.format(i_class))
    elif labels is None and n_classes &lt;= 2:
        labels = [&#39;&#39;]

    # call plot_roc function for both train and test sets
    fig_test = plot_roc(y_truth_test, y_score_test, labels, pos_label)
    fig_train = plot_roc(y_truth_train, y_score_train, labels, pos_label)
    axes_test = fig_test.get_axes()[0]
    axes_train = fig_train.get_axes()[0]

    # plot results together
    cmap = plt.cm.get_cmap(&#39;tab10&#39;)
    res = plt.figure()
    for i_roc, (roc_test, roc_train) in enumerate(zip(axes_test.lines, axes_train.lines)):
        if i_roc &lt; n_classes:
            if i_roc &gt; 0 and n_classes &lt;= 2:
                continue
            roc_auc_test = auc(roc_test.get_xdata(), roc_test.get_ydata())
            roc_auc_train = auc(roc_train.get_xdata(), roc_train.get_ydata())
            plt.plot(roc_train.get_xdata(), roc_train.get_ydata(), c=cmap(
                i_roc), alpha=0.3, lw=1, label=&#39;Train {0} (AUC = {1:.4f})&#39;.format(labels[i_roc], roc_auc_test))
            plt.plot(roc_test.get_xdata(), roc_test.get_ydata(), c=cmap(
                i_roc), lw=1, label=&#39;Test {0} (AUC = {1:.4f})&#39;.format(labels[i_roc], roc_auc_train))
    plt.xlabel(&#39;False Positive Rate&#39;, fontsize=12)
    plt.ylabel(&#39;True Positive Rate&#39;, fontsize=12)
    plt.legend(loc=&#39;lower right&#39;)
    plt.grid()

    plt.close(fig_test)
    plt.close(fig_train)
    del fig_test, fig_train, axes_test, axes_train

    return res


def plot_feature_imp(df_in, y_truth, model, n_sample=10000):
    &#34;&#34;&#34;
    Calculate the feature importance using the shap violin plot for
    each feature. The calculation is performed on a subsample of the
    input training/test set

    Input
    -------------------------------------------
    df_in: Pandas dataframe
    Training or test set dataframe

    y_truth: array
    Training or test set label
    model: trained model

    n_sample: int
    Number of candidates employed to fill
    the shap violin plots.
    If larger than the number of candidates
    in each class, minimum number of candidates
    in a given class used instead

    Output
    -------------------------------------------
    list of matplotlib objects with shap feature importance

    &#34;&#34;&#34;
    class_labels, class_counts = np.unique(y_truth, return_counts=True)
    n_classes = len(class_labels)
    for class_count in class_counts:
        if n_sample &gt; class_count:
            n_sample = class_count

    subs = []
    for i_class, class_lab in enumerate(class_labels):
        subs.append(df_in[y_truth == class_lab].sample(n_sample))

    df_subs = pd.concat(subs).sample(frac=1.)
    explainer = shap.TreeExplainer(model.get_original_model())
    shap_values = explainer.shap_values(df_subs, approximate=True)

    res = []
    if n_classes &lt;= 2:
        res.append(plt.figure(figsize=(18, 9)))
        shap.summary_plot(shap_values, df_subs, plot_size=(18, 9), show=False)
    else:
        for i_class in range(n_classes):
            res.append(plt.figure(figsize=(18, 9)))
            shap.summary_plot(shap_values[i_class], df_subs, plot_size=(18, 9), show=False)
        res.append(plt.figure(figsize=(18, 9)))
        shap.summary_plot(shap_values, df_subs, plot_type=&#39;bar&#39;, plot_size=(18, 9), show=False)

    return res


def plot_precision_recall(y_truth, y_score, labels=None, pos_label=None):
    &#34;&#34;&#34; Plot precision recall curve

    Input
    -------------------------------------
    y_truth: array
    True labels for the belonging class. If labels are not
    {0, 1, ..., N}, then pos_label should be explicitly given.

    y_score: array
    Estimated probabilities or decision function.

    pos_label : int or str
    The label of the positive class. When pos_label=None,
    if y_true is in {0, 1, ..., N}, pos_label is set to 1,
    otherwise an error will be raised.


    Output
    -------------------------------------
    matplotlib object with the precision
    recall curves

    &#34;&#34;&#34;
    # get number of classes
    n_classes = len(np.unique(y_truth))

    if (labels is None and n_classes &gt; 2) or (labels and len(labels) != n_classes):
        labels = []
        for i_class in range(n_classes):
            labels.append(&#39;class{}&#39;.format(i_class))

    res = plt.figure()
    if n_classes &lt;= 2:
        precision, recall, _ = precision_recall_curve(y_truth, y_score, pos_label=pos_label)
        plt.step(recall, precision, color=&#39;b&#39;, alpha=0.2, where=&#39;post&#39;)
        plt.fill_between(recall, precision, alpha=0.2, color=&#39;b&#39;, step=&#39;post&#39;)
        average_precision = average_precision_score(y_truth, y_score)
        plt.title(&#39;2-class Precision-Recall curve: AP={0:0.2f}&#39;.format(average_precision))
    else:
        cmap = plt.cm.get_cmap(&#39;tab10&#39;)
        precision, recall = (dict() for i_dict in range(2))
        # convert multi-class labels to multi-labels to obtain a curve for each class
        y_truth_multi = label_binarize(y_truth, classes=range(n_classes))
        for clas, lab in enumerate(labels):
            precision[clas], recall[clas], _ = precision_recall_curve(
                y_truth_multi[:, clas], y_score[:, clas], pos_label=pos_label)
            plt.step(recall[clas], precision[clas], color=cmap(clas), lw=1, where=&#39;post&#39;,
                     label=lab)
        # compute also micro average
        precision[&#39;micro&#39;], recall[&#39;micro&#39;], _ = precision_recall_curve(
            y_truth_multi.ravel(), y_score.ravel())
        plt.step(recall[&#39;micro&#39;], precision[&#39;micro&#39;], color=&#39;black&#39;, where=&#39;post&#39;,
                 linestyle=&#39;--&#39;, lw=1, label=&#39;average&#39;)
        average_precision = average_precision_score(y_truth_multi, y_score, average=&#39;micro&#39;)
        plt.title(&#39;Average precision score, micro-averaged over all classes: {0:0.2f}&#39;
                  .format(average_precision))

    plt.xlabel(&#39;Recall&#39;)
    plt.ylabel(&#39;Precision&#39;)
    plt.ylim([0.0, 1.05])
    plt.xlim([0.0, 1.0])
    if n_classes &gt; 2:
        plt.legend(loc=&#39;lower left&#39;)
        plt.grid()
    return res


def plot_learning_curves(model, data, n_points=10):
    &#34;&#34;&#34; Plot learning curves

    Input
    -------------------------------------
    model: xgboost or sklearn model

    data: list
    Contains respectively: training
    set dataframe, training label array,
    test set dataframe, test label array

    n_points: int
    Number of points used to sample the learning curves


    Output
    -------------------------------------
    matplotlib object with learning curves

    &#34;&#34;&#34;

    res = plt.figure()
    train_errors, test_errors = [], []
    min_cand = 100
    max_cand = len(data[0])
    step = int((max_cand-min_cand)/n_points)
    array_n_cand = np.arange(start=min_cand, stop=max_cand, step=step)
    for n_cand in array_n_cand:
        model.fit(data[0][:n_cand], data[1][:n_cand])
        y_train_predict = model.predict(data[0][:n_cand], output_margin=False)
        y_test_predict = model.predict(data[2], output_margin=False)
        train_errors.append(mean_squared_error(y_train_predict, data[1][:n_cand], multioutput=&#39;uniform_average&#39;))
        test_errors.append(mean_squared_error(y_test_predict, data[3], multioutput=&#39;uniform_average&#39;))
    plt.plot(array_n_cand, np.sqrt(train_errors), &#39;r&#39;, lw=1, label=&#39;Train&#39;)
    plt.plot(array_n_cand, np.sqrt(test_errors), &#39;b&#39;, lw=1, label=&#39;Test&#39;)
    plt.ylim([0, np.amax(np.sqrt(test_errors))*2])
    plt.xlabel(&#39;Training set size&#39;)
    plt.ylabel(&#39;RMSE&#39;)
    plt.grid()
    plt.legend(loc=&#39;best&#39;)

    return res</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="hipe4ml.plot_utils.plot_bdt_eff"><code class="name flex">
<span>def <span class="ident">plot_bdt_eff</span></span>(<span>threshold, eff_sig)</span>
</code></dt>
<dd>
<section class="desc"><p>Plot the BDT efficiency calculated with the function
bdt_efficiency_array() in analysis_utils</p>
<h2 id="input">Input</h2>
<dl>
<dt><strong><code>threshold</code></strong> :&ensp;<code>array</code></dt>
<dd>&nbsp;</dd>
</dl>
<p>Score threshold array</p>
<dl>
<dt><strong><code>eff_sig</code></strong> :&ensp;<code>array</code></dt>
<dd>&nbsp;</dd>
</dl>
<p>bdt efficiency array</p>
<h2 id="output">Output</h2>
<p>matplotlib object of the bdt efficiency as a
function of the threshold score</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def plot_bdt_eff(threshold, eff_sig):
    &#34;&#34;&#34;
    Plot the BDT efficiency calculated with the function
    bdt_efficiency_array() in analysis_utils

    Input
    -----------------------------------
    threshold: array
    Score threshold array

    eff_sig: array
    bdt efficiency array

    Output
    -----------------------------------
    matplotlib object of the bdt efficiency as a
    function of the threshold score

    &#34;&#34;&#34;
    res = plt.figure()
    plt.plot(threshold, eff_sig, &#39;r.&#39;, label=&#39;Signal efficiency&#39;)
    plt.legend()
    plt.xlabel(&#39;BDT Score&#39;)
    plt.ylabel(&#39;Efficiency&#39;)
    plt.title(&#39;Efficiency vs Score&#39;)
    plt.grid()
    return res</code></pre>
</details>
</dd>
<dt id="hipe4ml.plot_utils.plot_corr"><code class="name flex">
<span>def <span class="ident">plot_corr</span></span>(<span>list_of_df, columns, labels=None, **kwds)</span>
</code></dt>
<dd>
<section class="desc"><p>Calculate pairwise correlation between features for
each class (e.g. signal and background in case of binary
classification)</p>
<h2 id="input">Input</h2>
<dl>
<dt><strong><code>list_of_df</code></strong> :&ensp;<code>list</code></dt>
<dd>&nbsp;</dd>
</dl>
<p>Contains dataframes for each class</p>
<dl>
<dt><strong><code>columns</code></strong> :&ensp;<code>list</code></dt>
<dd>&nbsp;</dd>
<dt>Contains the name of the features you want to plot</dt>
<dt><strong><code>Example</code></strong> :&ensp;[<code>'dEdx'</code>, <code>'pT'</code>, <code>'ct'</code>]</dt>
<dd>&nbsp;</dd>
<dt><strong><code>labels</code></strong> :&ensp;<code>list</code></dt>
<dd>&nbsp;</dd>
</dl>
<p>Contains the labels to be displayed in the legend
If None the labels are class1, class2, &hellip;, classN</p>
<dl>
<dt><strong><code>**kwds</code></strong> :&ensp;<code>extra</code> <code>arguments</code> <code>are</code> <code>passed</code> <code>on</code> <code>to</code> <code>DataFrame.corr</code>()</dt>
<dd>&nbsp;</dd>
</dl>
<h2 id="output">Output</h2>
<p>list of matplotlib objects with the correlations
between the features for each class</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def plot_corr(list_of_df, columns, labels=None, **kwds):
    &#34;&#34;&#34;
    Calculate pairwise correlation between features for
    each class (e.g. signal and background in case of binary
    classification)

    Input
    -----------------------------------------------
    list_of_df: list
    Contains dataframes for each class

    columns: list
    Contains the name of the features you want to plot
    Example: [&#39;dEdx&#39;, &#39;pT&#39;, &#39;ct&#39;]

    labels: list
    Contains the labels to be displayed in the legend
    If None the labels are class1, class2, ..., classN

    **kwds: extra arguments are passed on to DataFrame.corr()

    Output
    ------------------------------------------------
    list of matplotlib objects with the correlations
    between the features for each class

    &#34;&#34;&#34;

    corr_mat = []
    for dfm in list_of_df:
        dfm = dfm[columns]
        corr_mat.append(dfm.corr(**kwds))

    if labels is None:
        labels = []
        if len(corr_mat) &gt; 2:
            for i_mat, _ in enumerate(corr_mat):
                labels.append(&#39;class{}&#39;.format(i_mat))
        else:
            labels.append(&#39;Signal&#39;)
            labels.append(&#39;Background&#39;)

    fig = []
    for mat, lab in zip(corr_mat, labels):
        fig.append(plt.figure(figsize=(8, 7)))
        grid = ImageGrid(fig[-1], 111, axes_pad=0.15, nrows_ncols=(1, 1), share_all=True,
                         cbar_location=&#39;right&#39;, cbar_mode=&#39;single&#39;, cbar_size=&#39;7%&#39;, cbar_pad=0.15)

        opts = {&#39;cmap&#39;: plt.get_cmap(
            &#39;coolwarm&#39;), &#39;vmin&#39;: -1, &#39;vmax&#39;: +1, &#39;snap&#39;: True}

        axs = grid[0]
        heatmap = axs.pcolor(mat, **opts)
        axs.set_title(lab, fontsize=14, fontweight=&#39;bold&#39;)

        lab = mat.columns.values

        # shift location of ticks to center of the bins
        axs.set_xticks(np.arange(len(lab)), minor=False)
        axs.set_yticks(np.arange(len(lab)), minor=False)
        axs.set_xticklabels(lab, minor=False, ha=&#39;left&#39;, rotation=90, fontsize=10)
        axs.set_yticklabels(lab, minor=False, va=&#39;bottom&#39;, fontsize=10)
        axs.tick_params(axis=&#39;both&#39;, which=&#39;both&#39;, direction=&#34;in&#34;)

        for tick in axs.xaxis.get_minor_ticks():
            tick.tick1line.set_markersize(0)
            tick.tick2line.set_markersize(0)
            tick.label1.set_horizontalalignment(&#39;center&#39;)

        axs.cax.colorbar(heatmap)

    return fig</code></pre>
</details>
</dd>
<dt id="hipe4ml.plot_utils.plot_distr"><code class="name flex">
<span>def <span class="ident">plot_distr</span></span>(<span>list_of_df, column=None, figsize=None, bins=50, log=False, labels=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Build a DataFrame and create a dataset for each class</p>
<p>Draw histogram of the DataFrame's series comparing the
distribution of each class.</p>
<h2 id="input">Input</h2>
<dl>
<dt><strong><code>list_of_df</code></strong> :&ensp;<code>list</code></dt>
<dd>&nbsp;</dd>
</dl>
<p>Contains a dataframe for each class</p>
<dl>
<dt><strong><code>column</code></strong> :&ensp;<code>list</code></dt>
<dd>&nbsp;</dd>
<dt>Contains the name of the features you want to plot</dt>
<dt><strong><code>Example</code></strong> :&ensp;[<code>'dEdx'</code>, <code>'pT'</code>, <code>'ct'</code>]</dt>
<dd>&nbsp;</dd>
<dt><strong><code>figsize</code></strong> :&ensp;<code>list</code></dt>
<dd>&nbsp;</dd>
</dl>
<p>The size in inches of the figure to create. Uses the value in matplotlib.rcParams by default.</p>
<dl>
<dt><strong><code>bins</code></strong> :&ensp;<code>int</code> or <code>sequence</code> of <code>scalars</code> or <code>str</code></dt>
<dd>&nbsp;</dd>
</dl>
<p>If bins is an int, it defines the number of equal-width
bins in the given range (10, by default). If bins is a
sequence, it defines a monotonically increasing array of
bin edges, including the rightmost edge, allowing for
non-uniform bin widths.</p>
<dl>
<dt><strong><code>log</code></strong> :&ensp;<code>Bool</code></dt>
<dd>&nbsp;</dd>
</dl>
<p>If True enable log scale plot</p>
<dl>
<dt><strong><code>labels</code></strong> :&ensp;<code>list</code></dt>
<dd>&nbsp;</dd>
</dl>
<p>Contains the labels to be displayed in the legend
If None the labels are class1, class2, &hellip;, classN</p>
<h2 id="output">Output</h2>
<p>array of matplotlib axes with the distributions
of the features for each class</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def plot_distr(list_of_df, column=None, figsize=None, bins=50, log=False, labels=None):
    &#34;&#34;&#34;
    Build a DataFrame and create a dataset for each class

    Draw histogram of the DataFrame&#39;s series comparing the
    distribution of each class.

    Input
    -----------------------------------------
    list_of_df: list
    Contains a dataframe for each class

    column: list
    Contains the name of the features you want to plot
    Example: [&#39;dEdx&#39;, &#39;pT&#39;, &#39;ct&#39;]

    figsize: list
    The size in inches of the figure to create. Uses the value in matplotlib.rcParams by default.

    bins: int or sequence of scalars or str
    If bins is an int, it defines the number of equal-width
    bins in the given range (10, by default). If bins is a
    sequence, it defines a monotonically increasing array of
    bin edges, including the rightmost edge, allowing for
    non-uniform bin widths.

    log: Bool
    If True enable log scale plot

    labels: list
    Contains the labels to be displayed in the legend
    If None the labels are class1, class2, ..., classN

    Output
    -----------------------------------------
    array of matplotlib axes with the distributions
    of the features for each class

    &#34;&#34;&#34;

    if column is not None:
        if not isinstance(column, (list, np.ndarray, Index)):
            column = [column]
        for dfm in list_of_df:
            dfm = dfm[column]

    if figsize is None:
        figsize = [20, 15]

    if labels is None:
        labels = [&#39;class{}&#39;.format(i_class) for i_class, _ in enumerate(list_of_df)]

    for i_class, (dfm, lab) in enumerate(zip(list_of_df, labels)):
        if i_class == 0:
            axes = dfm.hist(column=column, alpha=0.5, bins=bins, figsize=figsize, label=lab,
                            density=True, grid=False, log=log)
            axes = axes.flatten()
            axes = axes[:len(column)]
        else:
            dfm.hist(ax=axes, column=column, alpha=0.5, bins=bins, figsize=figsize, label=lab,
                     density=True, grid=False, log=log)
    for axs in axes:
        axs.set_ylabel(&#39;Counts (arb. units)&#39;)
    axes[-1].legend(loc=&#39;best&#39;)
    return axes</code></pre>
</details>
</dd>
<dt id="hipe4ml.plot_utils.plot_feature_imp"><code class="name flex">
<span>def <span class="ident">plot_feature_imp</span></span>(<span>df_in, y_truth, model, n_sample=10000)</span>
</code></dt>
<dd>
<section class="desc"><p>Calculate the feature importance using the shap violin plot for
each feature. The calculation is performed on a subsample of the
input training/test set</p>
<h2 id="input">Input</h2>
<dl>
<dt><strong><code>df_in</code></strong> :&ensp;<code>Pandas</code> <code>dataframe</code></dt>
<dd>&nbsp;</dd>
</dl>
<p>Training or test set dataframe</p>
<dl>
<dt><strong><code>y_truth</code></strong> :&ensp;<code>array</code></dt>
<dd>&nbsp;</dd>
<dt>Training or test set label</dt>
<dt><strong><code>model</code></strong> :&ensp;<code>trained</code> <code>model</code></dt>
<dd>&nbsp;</dd>
<dt><strong><code>n_sample</code></strong> :&ensp;<code>int</code></dt>
<dd>&nbsp;</dd>
</dl>
<p>Number of candidates employed to fill
the shap violin plots.
If larger than the number of candidates
in each class, minimum number of candidates
in a given class used instead</p>
<h2 id="output">Output</h2>
<p>list of matplotlib objects with shap feature importance</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def plot_feature_imp(df_in, y_truth, model, n_sample=10000):
    &#34;&#34;&#34;
    Calculate the feature importance using the shap violin plot for
    each feature. The calculation is performed on a subsample of the
    input training/test set

    Input
    -------------------------------------------
    df_in: Pandas dataframe
    Training or test set dataframe

    y_truth: array
    Training or test set label
    model: trained model

    n_sample: int
    Number of candidates employed to fill
    the shap violin plots.
    If larger than the number of candidates
    in each class, minimum number of candidates
    in a given class used instead

    Output
    -------------------------------------------
    list of matplotlib objects with shap feature importance

    &#34;&#34;&#34;
    class_labels, class_counts = np.unique(y_truth, return_counts=True)
    n_classes = len(class_labels)
    for class_count in class_counts:
        if n_sample &gt; class_count:
            n_sample = class_count

    subs = []
    for i_class, class_lab in enumerate(class_labels):
        subs.append(df_in[y_truth == class_lab].sample(n_sample))

    df_subs = pd.concat(subs).sample(frac=1.)
    explainer = shap.TreeExplainer(model.get_original_model())
    shap_values = explainer.shap_values(df_subs, approximate=True)

    res = []
    if n_classes &lt;= 2:
        res.append(plt.figure(figsize=(18, 9)))
        shap.summary_plot(shap_values, df_subs, plot_size=(18, 9), show=False)
    else:
        for i_class in range(n_classes):
            res.append(plt.figure(figsize=(18, 9)))
            shap.summary_plot(shap_values[i_class], df_subs, plot_size=(18, 9), show=False)
        res.append(plt.figure(figsize=(18, 9)))
        shap.summary_plot(shap_values, df_subs, plot_type=&#39;bar&#39;, plot_size=(18, 9), show=False)

    return res</code></pre>
</details>
</dd>
<dt id="hipe4ml.plot_utils.plot_learning_curves"><code class="name flex">
<span>def <span class="ident">plot_learning_curves</span></span>(<span>model, data, n_points=10)</span>
</code></dt>
<dd>
<section class="desc"><p>Plot learning curves</p>
<h2 id="input">Input</h2>
<dl>
<dt><strong><code>model</code></strong> :&ensp;<code>xgboost</code> or <code>sklearn</code> <code>model</code></dt>
<dd>&nbsp;</dd>
<dt><strong><code>data</code></strong> :&ensp;<code>list</code></dt>
<dd>&nbsp;</dd>
</dl>
<p>Contains respectively: training
set dataframe, training label array,
test set dataframe, test label array</p>
<dl>
<dt><strong><code>n_points</code></strong> :&ensp;<code>int</code></dt>
<dd>&nbsp;</dd>
</dl>
<p>Number of points used to sample the learning curves</p>
<h2 id="output">Output</h2>
<p>matplotlib object with learning curves</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def plot_learning_curves(model, data, n_points=10):
    &#34;&#34;&#34; Plot learning curves

    Input
    -------------------------------------
    model: xgboost or sklearn model

    data: list
    Contains respectively: training
    set dataframe, training label array,
    test set dataframe, test label array

    n_points: int
    Number of points used to sample the learning curves


    Output
    -------------------------------------
    matplotlib object with learning curves

    &#34;&#34;&#34;

    res = plt.figure()
    train_errors, test_errors = [], []
    min_cand = 100
    max_cand = len(data[0])
    step = int((max_cand-min_cand)/n_points)
    array_n_cand = np.arange(start=min_cand, stop=max_cand, step=step)
    for n_cand in array_n_cand:
        model.fit(data[0][:n_cand], data[1][:n_cand])
        y_train_predict = model.predict(data[0][:n_cand], output_margin=False)
        y_test_predict = model.predict(data[2], output_margin=False)
        train_errors.append(mean_squared_error(y_train_predict, data[1][:n_cand], multioutput=&#39;uniform_average&#39;))
        test_errors.append(mean_squared_error(y_test_predict, data[3], multioutput=&#39;uniform_average&#39;))
    plt.plot(array_n_cand, np.sqrt(train_errors), &#39;r&#39;, lw=1, label=&#39;Train&#39;)
    plt.plot(array_n_cand, np.sqrt(test_errors), &#39;b&#39;, lw=1, label=&#39;Test&#39;)
    plt.ylim([0, np.amax(np.sqrt(test_errors))*2])
    plt.xlabel(&#39;Training set size&#39;)
    plt.ylabel(&#39;RMSE&#39;)
    plt.grid()
    plt.legend(loc=&#39;best&#39;)

    return res</code></pre>
</details>
</dd>
<dt id="hipe4ml.plot_utils.plot_output_train_test"><code class="name flex">
<span>def <span class="ident">plot_output_train_test</span></span>(<span>model, data, bins=80, raw=True, labels=None, **kwds)</span>
</code></dt>
<dd>
<section class="desc"><p>Plot the BDT output distributions for each class and output
both for training and test set.</p>
<h2 id="input">Input</h2>
<dl>
<dt><strong><code>model</code></strong> :&ensp;<code>xgboost</code> or <code>sklearn</code> <code>model</code></dt>
<dd>&nbsp;</dd>
<dt><strong><code>data</code></strong> :&ensp;<code>list</code></dt>
<dd>&nbsp;</dd>
</dl>
<p>Contains respectively: training
set dataframe, training label array,
test set dataframe, test label array</p>
<dl>
<dt><strong><code>bins</code></strong> :&ensp;<code>int</code> or <code>sequence</code> of <code>scalars</code> or <code>str</code></dt>
<dd>&nbsp;</dd>
</dl>
<p>If bins is an int, it defines the number of equal-width
bins in the given range (10, by default). If bins is a
sequence, it defines a monotonically increasing array of
bin edges, including the rightmost edge, allowing for
non-uniform bin widths.
If bins is a string, it defines the method used to
calculate the optimal bin width, as defined by
np.histogram_bin_edges:
<a href="https://docs.scipy.org/doc/numpy/reference/generated">https://docs.scipy.org/doc/numpy/reference/generated</a>
/numpy.histogram_bin_edges.html#numpy.histogram_bin_edges</p>
<dl>
<dt><strong><code>raw</code></strong> :&ensp;<code>Bool</code></dt>
<dd>&nbsp;</dd>
</dl>
<p>If true enables the raw untransformed margin value</p>
<dl>
<dt><strong><code>labels</code></strong> :&ensp;<code>list</code></dt>
<dd>&nbsp;</dd>
</dl>
<p>Contains the labels to be displayed in the legend
If None the labels are class1, class2, &hellip;, classN</p>
<dl>
<dt><strong><code>**kwds</code></strong> :&ensp;<code>extra</code> <code>arguments</code> <code>are</code> <code>passed</code> <code>on</code> <code>to</code> <code>plt.hist</code>()</dt>
<dd>&nbsp;</dd>
</dl>
<h2 id="output">Output</h2>
<p>list of matplotlib objects with the BDT output
distributions for each class</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def plot_output_train_test(
        model, data, bins=80, raw=True, labels=None, **kwds):
    &#34;&#34;&#34;
    Plot the BDT output distributions for each class and output
    both for training and test set.

    Input
    ----------------------------------------
    model: xgboost or sklearn model

    data: list
    Contains respectively: training
    set dataframe, training label array,
    test set dataframe, test label array

    bins: int or sequence of scalars or str
    If bins is an int, it defines the number of equal-width
    bins in the given range (10, by default). If bins is a
    sequence, it defines a monotonically increasing array of
    bin edges, including the rightmost edge, allowing for
    non-uniform bin widths.
    If bins is a string, it defines the method used to
    calculate the optimal bin width, as defined by
    np.histogram_bin_edges:
    https://docs.scipy.org/doc/numpy/reference/generated
    /numpy.histogram_bin_edges.html#numpy.histogram_bin_edges

    raw: Bool
    If true enables the raw untransformed margin value

    labels: list
    Contains the labels to be displayed in the legend
    If None the labels are class1, class2, ..., classN

    **kwds: extra arguments are passed on to plt.hist()

    Output
    ----------------------------------------
    list of matplotlib objects with the BDT output
    distributions for each class


    &#34;&#34;&#34;
    class_labels = np.unique(data[1])
    n_classes = len(class_labels)

    prediction = []
    for xxx, yyy in ((data[0], data[1]), (data[2], data[3])):
        for class_lab in class_labels:
            prediction.append(model.predict(xxx[yyy == class_lab], output_margin=raw))

    low = min(np.min(d) for d in prediction)
    high = max(np.max(d) for d in prediction)
    low_high = (low, high)

    res = []
    scale = 1.
    # only one figure in case of binary classification
    if n_classes &lt;= 2:
        labels = [&#39;Signal&#39;, &#39;Background&#39;] if labels is None else labels
        colors = [&#39;b&#39;, &#39;r&#39;]
        res.append(plt.figure())
        for i_class in range(n_classes):
            plt.hist(prediction[i_class], color=colors[i_class], alpha=0.5, range=low_high, bins=bins,
                     histtype=&#39;stepfilled&#39;, label=&#39;{} pdf Training Set&#39;.format(labels[i_class]), **kwds)

            hist, bins = np.histogram(prediction[i_class+2], bins=bins, range=low_high, **kwds)
            if &#39;density&#39; in kwds and kwds[&#39;density&#39;]:
                err = np.sqrt(hist * len(prediction[i_class])) / len(prediction[i_class+2])
            else:
                scale = len(prediction[i_class]) / sum(hist)
                err = np.sqrt(hist) * scale
            center = (bins[:-1] + bins[1:]) / 2
            plt.errorbar(center, hist * scale, yerr=err, fmt=&#39;o&#39;,
                         c=colors[i_class], label=&#39;{} pdf Test Set&#39;.format(labels[i_class]))

        plt.xlabel(&#39;BDT output&#39;, fontsize=13, ha=&#39;right&#39;, position=(1, 20))
        plt.ylabel(r&#39;                                Counts (arb. units)&#39;, fontsize=13)
        plt.legend(frameon=False, fontsize=12)
    # n figures in case of multi-classification with n classes
    else:
        labels = [&#39;class{}&#39;.format(i_class) for i_class, _ in enumerate(class_labels)] if labels is None else labels
        cmap = plt.cm.get_cmap(&#39;tab10&#39;)
        for i_output, _ in enumerate(class_labels):
            res.append(plt.figure())
            for i_class, lab in enumerate(labels):
                plt.hist(prediction[i_class][:, i_output], alpha=0.5, range=low_high, bins=bins,
                         color=cmap(i_class), histtype=&#39;stepfilled&#39;,
                         label=&#39;{} pdf Training Set&#39;.format(lab), **kwds)

                hist, bins = np.histogram(
                    prediction[i_class+n_classes][:, i_output], bins=bins, range=low_high, **kwds)
                if &#39;density&#39; in kwds and kwds[&#39;density&#39;]:
                    err = np.sqrt(hist * len(prediction[i_class][:, i_output])) / len(
                        prediction[i_class+n_classes][:, i_output])
                else:
                    scale = len(prediction[i_class][:, i_output]) / sum(hist)
                    err = np.sqrt(hist) * scale
                center = (bins[:-1] + bins[1:]) / 2

                plt.errorbar(center, hist * scale, yerr=err, fmt=&#39;o&#39;,
                             c=cmap(i_class), label=&#39;{} pdf Test Set&#39;.format(lab))

            plt.xlabel(&#39;BDT output for {}&#39;.format(labels[i_output]), fontsize=13, ha=&#39;right&#39;,
                       position=(1, 20))
            plt.ylabel(r&#39;                                Counts (arb. units)&#39;, fontsize=13)
            plt.legend(frameon=False, fontsize=12)

    return res</code></pre>
</details>
</dd>
<dt id="hipe4ml.plot_utils.plot_precision_recall"><code class="name flex">
<span>def <span class="ident">plot_precision_recall</span></span>(<span>y_truth, y_score, labels=None, pos_label=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Plot precision recall curve</p>
<h2 id="input">Input</h2>
<dl>
<dt><strong><code>y_truth</code></strong> :&ensp;<code>array</code></dt>
<dd>&nbsp;</dd>
</dl>
<p>True labels for the belonging class. If labels are not
{0, 1, &hellip;, N}, then pos_label should be explicitly given.</p>
<dl>
<dt><strong><code>y_score</code></strong> :&ensp;<code>array</code></dt>
<dd>&nbsp;</dd>
</dl>
<p>Estimated probabilities or decision function.</p>
<dl>
<dt><strong><code>pos_label</code></strong> :&ensp;<code>int</code> or <code>str</code></dt>
<dd>&nbsp;</dd>
</dl>
<p>The label of the positive class. When pos_label=None,
if y_true is in {0, 1, &hellip;, N}, pos_label is set to 1,
otherwise an error will be raised.</p>
<h2 id="output">Output</h2>
<p>matplotlib object with the precision
recall curves</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def plot_precision_recall(y_truth, y_score, labels=None, pos_label=None):
    &#34;&#34;&#34; Plot precision recall curve

    Input
    -------------------------------------
    y_truth: array
    True labels for the belonging class. If labels are not
    {0, 1, ..., N}, then pos_label should be explicitly given.

    y_score: array
    Estimated probabilities or decision function.

    pos_label : int or str
    The label of the positive class. When pos_label=None,
    if y_true is in {0, 1, ..., N}, pos_label is set to 1,
    otherwise an error will be raised.


    Output
    -------------------------------------
    matplotlib object with the precision
    recall curves

    &#34;&#34;&#34;
    # get number of classes
    n_classes = len(np.unique(y_truth))

    if (labels is None and n_classes &gt; 2) or (labels and len(labels) != n_classes):
        labels = []
        for i_class in range(n_classes):
            labels.append(&#39;class{}&#39;.format(i_class))

    res = plt.figure()
    if n_classes &lt;= 2:
        precision, recall, _ = precision_recall_curve(y_truth, y_score, pos_label=pos_label)
        plt.step(recall, precision, color=&#39;b&#39;, alpha=0.2, where=&#39;post&#39;)
        plt.fill_between(recall, precision, alpha=0.2, color=&#39;b&#39;, step=&#39;post&#39;)
        average_precision = average_precision_score(y_truth, y_score)
        plt.title(&#39;2-class Precision-Recall curve: AP={0:0.2f}&#39;.format(average_precision))
    else:
        cmap = plt.cm.get_cmap(&#39;tab10&#39;)
        precision, recall = (dict() for i_dict in range(2))
        # convert multi-class labels to multi-labels to obtain a curve for each class
        y_truth_multi = label_binarize(y_truth, classes=range(n_classes))
        for clas, lab in enumerate(labels):
            precision[clas], recall[clas], _ = precision_recall_curve(
                y_truth_multi[:, clas], y_score[:, clas], pos_label=pos_label)
            plt.step(recall[clas], precision[clas], color=cmap(clas), lw=1, where=&#39;post&#39;,
                     label=lab)
        # compute also micro average
        precision[&#39;micro&#39;], recall[&#39;micro&#39;], _ = precision_recall_curve(
            y_truth_multi.ravel(), y_score.ravel())
        plt.step(recall[&#39;micro&#39;], precision[&#39;micro&#39;], color=&#39;black&#39;, where=&#39;post&#39;,
                 linestyle=&#39;--&#39;, lw=1, label=&#39;average&#39;)
        average_precision = average_precision_score(y_truth_multi, y_score, average=&#39;micro&#39;)
        plt.title(&#39;Average precision score, micro-averaged over all classes: {0:0.2f}&#39;
                  .format(average_precision))

    plt.xlabel(&#39;Recall&#39;)
    plt.ylabel(&#39;Precision&#39;)
    plt.ylim([0.0, 1.05])
    plt.xlim([0.0, 1.0])
    if n_classes &gt; 2:
        plt.legend(loc=&#39;lower left&#39;)
        plt.grid()
    return res</code></pre>
</details>
</dd>
<dt id="hipe4ml.plot_utils.plot_roc"><code class="name flex">
<span>def <span class="ident">plot_roc</span></span>(<span>y_truth, y_score, labels=None, pos_label=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Calculate and plot the roc curve</p>
<h2 id="input">Input</h2>
<dl>
<dt><strong><code>y_truth</code></strong> :&ensp;<code>array</code></dt>
<dd>&nbsp;</dd>
</dl>
<p>True labels for the belonging class. If labels are not
{0, 1, &hellip;, N}, then pos_label should be explicitly given.</p>
<dl>
<dt><strong><code>y_score</code></strong> :&ensp;<code>array</code></dt>
<dd>&nbsp;</dd>
</dl>
<p>Target scores, can either be probability estimates
of the positive class, confidence values, or
non-thresholded measure of decisions (as returned
by “decision_function” on some classifiers).</p>
<dl>
<dt><strong><code>labels</code></strong> :&ensp;<code>list</code></dt>
<dd>&nbsp;</dd>
</dl>
<p>Contains the labels to be displayed in the legend
If None the labels are class1, class2, &hellip;, classN</p>
<dl>
<dt><strong><code>pos_label</code></strong> :&ensp;<code>int</code> or <code>str</code></dt>
<dd>&nbsp;</dd>
</dl>
<p>The label of the positive class. When pos_label=None,
if y_true is in {0, 1, &hellip;, N}, pos_label is set to 1,
otherwise an error will be raised.</p>
<h2 id="output">Output</h2>
<p>matplotlib object with the roc curves</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def plot_roc(y_truth, y_score, labels=None, pos_label=None):
    &#34;&#34;&#34;
    Calculate and plot the roc curve

    Input
    -------------------------------------

    y_truth : array
    True labels for the belonging class. If labels are not
    {0, 1, ..., N}, then pos_label should be explicitly given.

    y_score : array
    Target scores, can either be probability estimates
    of the positive class, confidence values, or
    non-thresholded measure of decisions (as returned
    by “decision_function” on some classifiers).

    labels: list
    Contains the labels to be displayed in the legend
    If None the labels are class1, class2, ..., classN

    pos_label : int or str
    The label of the positive class. When pos_label=None,
    if y_true is in {0, 1, ..., N}, pos_label is set to 1,
    otherwise an error will be raised.



    Output
    -------------------------------------
    matplotlib object with the roc curves

    &#34;&#34;&#34;
    # get number of classes
    n_classes = len(np.unique(y_truth))

    if (labels is None and n_classes &gt; 2) or (labels and len(labels) != n_classes):
        labels = []
        for i_class in range(n_classes):
            labels.append(&#39;class{}&#39;.format(i_class))

    res = plt.figure()
    if n_classes &lt;= 2:
        fpr, tpr, _ = roc_curve(y_truth, y_score, pos_label=pos_label)
        roc_auc = auc(fpr, tpr)
        plt.plot(fpr, tpr, lw=1, label=&#39;ROC (area = %0.4f)&#39; % (roc_auc))
    else:
        cmap = plt.cm.get_cmap(&#39;tab10&#39;)
        fpr, tpr, roc_auc = (dict() for i_dict in range(3))
        # convert multi-class labels to multi-labels to obtain roc curves
        y_truth_multi = label_binarize(y_truth, classes=range(n_classes))
        for clas, lab in enumerate(labels):
            fpr[clas], tpr[clas], _ = roc_curve(y_truth_multi[:, clas], y_score[:, clas])
            roc_auc[clas] = auc(fpr[clas], tpr[clas])
            plt.plot(fpr[clas], tpr[clas], lw=1, c=cmap(clas),
                     label=&#39;{0} (AUC = {1:.4f})&#39;.format(lab, roc_auc[clas]))
        # compute also micro average
        fpr[&#39;micro&#39;], tpr[&#39;micro&#39;], _ = roc_curve(y_truth_multi.ravel(), y_score.ravel())
        roc_auc[&#39;micro&#39;] = auc(fpr[&#39;micro&#39;], tpr[&#39;micro&#39;])
        plt.plot(fpr[&#39;micro&#39;], tpr[&#39;micro&#39;], lw=1, linestyle=&#39;--&#39;, c=&#39;black&#39;,
                 label=&#39;average (AUC = {:.4f})&#39;.format(roc_auc[&#39;micro&#39;]))

    plt.plot([0, 1], [0, 1], &#39;-.&#39;, color=(0.6, 0.6, 0.6), label=&#39;Luck&#39;)
    plt.xlim([-0.05, 1.05])
    plt.ylim([-0.05, 1.05])
    plt.xlabel(&#39;False Positive Rate&#39;, fontsize=12)
    plt.ylabel(&#39;True Positive Rate&#39;, fontsize=12)
    plt.legend(loc=&#39;lower right&#39;)
    plt.grid()
    return res</code></pre>
</details>
</dd>
<dt id="hipe4ml.plot_utils.plot_roc_train_test"><code class="name flex">
<span>def <span class="ident">plot_roc_train_test</span></span>(<span>y_truth_test, y_score_test, y_truth_train, y_score_train, labels=None, pos_label=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Calculate and plot the roc curve for test and train sets</p>
<h2 id="input">Input</h2>
<dl>
<dt><strong><code>y_truth_test</code></strong> :&ensp;<code>array</code></dt>
<dd>&nbsp;</dd>
</dl>
<p>True labels for the belonging class of the test set.
If labels are not {0, 1, &hellip;, N}, then pos_label
should be explicitly given.</p>
<dl>
<dt><strong><code>y_score_test</code></strong> :&ensp;<code>array</code></dt>
<dd>&nbsp;</dd>
</dl>
<p>Target scores for the test set, can either be probability
estimates of the positive class, confidence values, or
non-thresholded measure of decisions (as returned
by “decision_function” on some classifiers).</p>
<dl>
<dt><strong><code>y_truth_train</code></strong> :&ensp;<code>array</code></dt>
<dd>&nbsp;</dd>
</dl>
<p>True labels for the belonging class of the train set.
If labels are not {0, 1, &hellip;, N}, then pos_label
should be explicitly given.</p>
<dl>
<dt><strong><code>y_score_train</code></strong> :&ensp;<code>array</code></dt>
<dd>&nbsp;</dd>
</dl>
<p>Target scores for the train set, can either be probability
estimates of the positive class, confidence values, or
non-thresholded measure of decisions (as returned
by “decision_function” on some classifiers).</p>
<dl>
<dt><strong><code>labels</code></strong> :&ensp;<code>list</code></dt>
<dd>&nbsp;</dd>
</dl>
<p>Contains the labels to be displayed in the legend
If None the labels are class1, class2, &hellip;, classN</p>
<dl>
<dt><strong><code>pos_label</code></strong> :&ensp;<code>int</code> or <code>str</code></dt>
<dd>&nbsp;</dd>
</dl>
<p>The label of the positive class. When pos_label=None,
if y_true_ is in {0, 1, &hellip;, N}, pos_label is set to 1,
otherwise an error will be raised.</p>
<h2 id="output">Output</h2>
<p>matplotlib object with the roc curves</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def plot_roc_train_test(y_truth_test, y_score_test, y_truth_train, y_score_train, labels=None, pos_label=None):
    &#34;&#34;&#34;
    Calculate and plot the roc curve for test and train sets

    Input
    -------------------------------------

    y_truth_test : array
    True labels for the belonging class of the test set.
    If labels are not {0, 1, ..., N}, then pos_label
    should be explicitly given.

    y_score_test : array
    Target scores for the test set, can either be probability
    estimates of the positive class, confidence values, or
    non-thresholded measure of decisions (as returned
    by “decision_function” on some classifiers).

    y_truth_train : array
    True labels for the belonging class of the train set.
    If labels are not {0, 1, ..., N}, then pos_label
    should be explicitly given.

    y_score_train : array
    Target scores for the train set, can either be probability
    estimates of the positive class, confidence values, or
    non-thresholded measure of decisions (as returned
    by “decision_function” on some classifiers).

    labels: list
    Contains the labels to be displayed in the legend
    If None the labels are class1, class2, ..., classN

    pos_label : int or str
    The label of the positive class. When pos_label=None,
    if y_true_ is in {0, 1, ..., N}, pos_label is set to 1,
    otherwise an error will be raised.



    Output
    -------------------------------------
    matplotlib object with the roc curves

    &#34;&#34;&#34;
    # get number of classes
    n_classes = len(np.unique(y_truth_test))

    if (labels is None and n_classes &gt; 2) or (labels and len(labels) != n_classes):
        labels = []
        for i_class in range(n_classes):
            labels.append(&#39;class{}&#39;.format(i_class))
    elif labels is None and n_classes &lt;= 2:
        labels = [&#39;&#39;]

    # call plot_roc function for both train and test sets
    fig_test = plot_roc(y_truth_test, y_score_test, labels, pos_label)
    fig_train = plot_roc(y_truth_train, y_score_train, labels, pos_label)
    axes_test = fig_test.get_axes()[0]
    axes_train = fig_train.get_axes()[0]

    # plot results together
    cmap = plt.cm.get_cmap(&#39;tab10&#39;)
    res = plt.figure()
    for i_roc, (roc_test, roc_train) in enumerate(zip(axes_test.lines, axes_train.lines)):
        if i_roc &lt; n_classes:
            if i_roc &gt; 0 and n_classes &lt;= 2:
                continue
            roc_auc_test = auc(roc_test.get_xdata(), roc_test.get_ydata())
            roc_auc_train = auc(roc_train.get_xdata(), roc_train.get_ydata())
            plt.plot(roc_train.get_xdata(), roc_train.get_ydata(), c=cmap(
                i_roc), alpha=0.3, lw=1, label=&#39;Train {0} (AUC = {1:.4f})&#39;.format(labels[i_roc], roc_auc_test))
            plt.plot(roc_test.get_xdata(), roc_test.get_ydata(), c=cmap(
                i_roc), lw=1, label=&#39;Test {0} (AUC = {1:.4f})&#39;.format(labels[i_roc], roc_auc_train))
    plt.xlabel(&#39;False Positive Rate&#39;, fontsize=12)
    plt.ylabel(&#39;True Positive Rate&#39;, fontsize=12)
    plt.legend(loc=&#39;lower right&#39;)
    plt.grid()

    plt.close(fig_test)
    plt.close(fig_train)
    del fig_test, fig_train, axes_test, axes_train

    return res</code></pre>
</details>
</dd>
</dl>
</section>
<section>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="hipe4ml" href="index.html">hipe4ml</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="hipe4ml.plot_utils.plot_bdt_eff" href="#hipe4ml.plot_utils.plot_bdt_eff">plot_bdt_eff</a></code></li>
<li><code><a title="hipe4ml.plot_utils.plot_corr" href="#hipe4ml.plot_utils.plot_corr">plot_corr</a></code></li>
<li><code><a title="hipe4ml.plot_utils.plot_distr" href="#hipe4ml.plot_utils.plot_distr">plot_distr</a></code></li>
<li><code><a title="hipe4ml.plot_utils.plot_feature_imp" href="#hipe4ml.plot_utils.plot_feature_imp">plot_feature_imp</a></code></li>
<li><code><a title="hipe4ml.plot_utils.plot_learning_curves" href="#hipe4ml.plot_utils.plot_learning_curves">plot_learning_curves</a></code></li>
<li><code><a title="hipe4ml.plot_utils.plot_output_train_test" href="#hipe4ml.plot_utils.plot_output_train_test">plot_output_train_test</a></code></li>
<li><code><a title="hipe4ml.plot_utils.plot_precision_recall" href="#hipe4ml.plot_utils.plot_precision_recall">plot_precision_recall</a></code></li>
<li><code><a title="hipe4ml.plot_utils.plot_roc" href="#hipe4ml.plot_utils.plot_roc">plot_roc</a></code></li>
<li><code><a title="hipe4ml.plot_utils.plot_roc_train_test" href="#hipe4ml.plot_utils.plot_roc_train_test">plot_roc_train_test</a></code></li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.7.2</a>.</p>
</footer>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad()</script>
</body>
</html>